{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZStr8Th4jBq"
      },
      "source": [
        "# Assignment 4: Ranking & Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN0aEQUD4mwy"
      },
      "source": [
        "## General guidelines\n",
        "\n",
        "This notebook contains considerable amount of code to help you complete this assignment. Your task is to implement any missing parts of the code and answer any questions (if exist) within this notebook. This will require understanding the existing code, may require reading about packages being used, reading additional resources, and maybe even going over your notes from class ðŸ˜±\n",
        "\n",
        "**Evaluation and auto-grading**: Your submissions will be evaluated using both automatic and manual grading. Code parts for implementation are marked with a comment `# YOUR CODE HERE`, and usually followed by cell(s) containing automatic tests that evaluate the correctness of your answer. Additionaly, staff will manually assess your submission. Any automatic tests that did not run due to your notebook timing out **will automatically receive 0 points**. The execution time excludes initial data download, which will already exist in the testing environment. The staff reserves the right to **modify any grade provided by the auto-grader** as well as to **execute additional tests not provided to you**. It is also important to note that **auto-graded cells only result in full or no credit**. In other words, you must pass all tests implemented in a test cell in order to get the credit for it, and passing some, but not all, of the tests in a test cell will not earn you any points for that cell. \n",
        "\n",
        "**Submission**: Unless specified otherwise, you need to upload this notebook file **with your ID as the file name**, e.g. 012345678.ipynb, to the assignment on Moodle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FLuP8v74n4i"
      },
      "source": [
        "# Tasks\n",
        "\n",
        "In this assignment, we are going to build a complete retrieval system using the inverted index (assignment 2)\n",
        "\n",
        "**Your tasks in this assignment are:**\n",
        "\n",
        "1. (25 Points) Implement ranking using TF-IDF and BM25.\n",
        "2. (20 Points) Implement search/retrieval using the inverted index. Implement compound ranking based on title and body. \n",
        "4. (15 Points) Using weighting of title and body scores. Provide three examples of mistakes that the model is making and explanations for why, and describe how you will change the model based on these observations.\n",
        "5. (25 Points) Implement the multiple metrics, including precision, recall, NDCG, MRR, and more.\n",
        "6. (5 Points) Optimize your ranking function for MAP@100 using the training set. Validate that the final submitted model passes the test of MAP@100 on the test set. \n",
        "7. (10 Points) Plot graphs. First, plot metrics scores for the different queries. Second, plot metrics scores with varying values of k. (e.g., MRR@1, MRR@3, MRR@5, etc.')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXsCKOx2FTn2"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 324,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "QGIQb5FHFSDH",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e4d131fb4a3e9c60c954e1c12f01baa9",
          "grade": false,
          "grade_id": "cell-978e0be3020a3ddc",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "74f11145-032c-4058-db2f-acfa17c5fc77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from collections import defaultdict,Counter\n",
        "import re\n",
        "import nltk\n",
        "import pickle\n",
        "import numpy as np\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from tqdm import tqdm\n",
        "import operator\n",
        "from itertools import islice,count\n",
        "from contextlib import closing\n",
        "\n",
        "import json\n",
        "from io import StringIO\n",
        "from pathlib import Path\n",
        "from operator import itemgetter\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 325,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "HYUM7NJ8GmTX",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b4f8b4729a7248ce5705c2c11ae7dbeb",
          "grade": false,
          "grade_id": "cell-da830e169e10030e",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "f4ba2502-7389-4f96-9234-30e926803669"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-18 08:37:59--  http://ir.dcs.gla.ac.uk/resources/test_collections/cran/cran.tar.gz\n",
            "Resolving ir.dcs.gla.ac.uk (ir.dcs.gla.ac.uk)... 130.209.240.253\n",
            "Connecting to ir.dcs.gla.ac.uk (ir.dcs.gla.ac.uk)|130.209.240.253|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 506960 (495K) [application/gzip]\n",
            "Saving to: â€˜cran.tar.gz.1â€™\n",
            "\n",
            "cran.tar.gz.1       100%[===================>] 495.08K   997KB/s    in 0.5s    \n",
            "\n",
            "2021-12-18 08:38:00 (997 KB/s) - â€˜cran.tar.gz.1â€™ saved [506960/506960]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://ir.dcs.gla.ac.uk/resources/test_collections/cran/cran.tar.gz\n",
        "!tar -xf cran.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiXcPL-9qH8g"
      },
      "source": [
        "## Tasks 1: Implement ranking using TF-IDF and BM25 (25 points).\n",
        "In this task, you need to implement TF-IDF and BM25. At this point, for simplicity, we do not use an inverted index. However, later on in this assignment, we will.\n",
        "\n",
        "Implementation remarks:\n",
        "* TF-IDF: use [sklearn TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). To deal with stopwords use the argument `stop_words`. (5 points)\n",
        "\n",
        "TfidfVectorizer suggests handling with additional parameters, as you can read in the documentation. Make sure you read about them.\n",
        "\n",
        "* Cosine Similarity: use [sklearn implementation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html). (5 points)\n",
        "\n",
        "* BM25: Implement a BM25 version according to the provided skeleton **without the use of packages**. 10 points as follows:\n",
        "    * Prepare the data and filter stopwords. (5 points)\n",
        "    * Implement two functions at BM25 class. (5 points)\n",
        "\n",
        "* Ranking: implement topN functionallity (5 points)\n",
        "\n",
        "**Later in this assignment, we will write code for TF-IDF and BM25 that utilize the inverted index.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 326,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "pbkM4LjQzp1S",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d3f9914f6ec64219ec9558b553f38f11",
          "grade": false,
          "grade_id": "cell-5be4d8ec4f103392",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# set of documents\n",
        "data = ['The sky is blue and we can see the blue sun.',\n",
        "        'The sun is bright and yellow.',\n",
        "        'here comes the blue sun',\n",
        "        'Lucy in the sky with diamonds and you can see the sun in the sky',\n",
        "        'sun sun blue sun here we come',\n",
        "        'Lucy likes blue bright diamonds']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXG9bNEdqOZX"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-GxCnRuz4rE"
      },
      "source": [
        "**YOUR TASK (5 POINTS):**  Complete the implementation of `tf_idf_scores`, which calculates the tfidf for each word in a single document utilizing TfidfVectorizer via sklearn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 327,
      "metadata": {
        "deletable": false,
        "id": "NahU25YgsV8f",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a90519574cfa59d445c176a1dcd7cd35",
          "grade": false,
          "grade_id": "cell-577ba8cca719f1d9",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def tf_idf_scores(data):\n",
        "    \"\"\"\n",
        "    This function calculates the tfidf for each word in a single document utilizing TfidfVectorizer via sklearn.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "      data: list of strings.\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "      Two objects as follows:\n",
        "                                a) DataFrame, documents as rows (i.e., 0,1,2,3, etc'), terms as columns ('bird','bright', etc').\n",
        "                                b) TfidfVectorizer object.\n",
        "\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    tfidf_vectorizer=TfidfVectorizer(analyzer='word',stop_words='english',use_idf=True) \n",
        "    tfidfvectorizer=tfidf_vectorizer.fit_transform(data)\n",
        "    dataframe = pd.DataFrame(data = tfidfvectorizer.toarray(),columns= tfidf_vectorizer.get_feature_names_out())\n",
        "    yield dataframe\n",
        "    yield tfidf_vectorizer\n",
        "    \n",
        "df_tfidfvect, tfidfvectorizer = tf_idf_scores(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 328,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "-P03jq5Z1POS",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ffbb01fd8c5be0fb06ecfa05555f2d76",
          "grade": true,
          "grade_id": "cell-ca8a4ee9a8616265",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "#tests\n",
        "\n",
        "assert df_tfidfvect.shape[1] == 10 and df_tfidfvect.shape[0] == 6\n",
        "assert 'is' not in df_tfidfvect.columns and 'we' not in df_tfidfvect.columns\n",
        "assert 'sun' in df_tfidfvect.columns and 'yellow' in df_tfidfvect.columns\n",
        "assert round(df_tfidfvect.max(),3).max() == 0.798\n",
        "assert np.count_nonzero(df_tfidfvect) == 21\n",
        "assert type(tfidfvectorizer) == TfidfVectorizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1DA10t1Fa4f"
      },
      "source": [
        "Now, upon existing TF-IDF matrix we are seeking for the similarity for given new queries. \n",
        "First we need to convert the **new** queries into a vector format. Therefore we use transform instead of fit_transform.\n",
        "\n",
        "Next, we would like to calculate the cosine similarity between queires and documents.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 329,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "l1mSzpDRxs2Q",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "8623e7014dd9df699215b124e110df0a",
          "grade": false,
          "grade_id": "cell-cb743342253481b9",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "queries = ['look the the blue sky', 'He likes the blue the sun','Lucy likes blue sky with diamonds']\n",
        "queries_vector = tfidfvectorizer.transform(queries)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDBJ24TQ8BD8"
      },
      "source": [
        "Now, lets calculate the cosine similarity utilizing sklearn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RdQmZ-ZBFMT"
      },
      "source": [
        "**YOUR TASK (5 POINTS):**  Complete the implementation of `cosine_sim_using_sklearn`.\n",
        "You need to compute the similarity between the queries and the given documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 330,
      "metadata": {
        "deletable": false,
        "id": "V_8kmozHFuOI",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e91549ea204726c3ccf37e4501698a84",
          "grade": false,
          "grade_id": "cell-c14779ef9f634445",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def cosine_sim_using_sklearn(queries,tfidf):\n",
        "    \"\"\"\n",
        "    In this function you need to utilize the cosine_similarity function from sklearn.\n",
        "    You need to compute the similarity between the queries and the given documents.\n",
        "    This function will return a DataFrame in the following shape: (# of queries, # of documents).\n",
        "    Each value in the DataFrame will represent the cosine_similarity between given query and document.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "      queries: sparse matrix represent the queries after transformation of tfidfvectorizer.\n",
        "      documents: sparse matrix represent the documents.\n",
        "      \n",
        "    Returns:\n",
        "    --------\n",
        "      DataFrame: This function will return a DataFrame in the following shape: (# of queries, # of documents).\n",
        "      Each value in the DataFrame will represent the cosine_similarity between given query and document.\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    dataframe = pd.DataFrame(data = queries_vector.toarray(),columns= tfidfvectorizer.get_feature_names_out())\n",
        "    return cosine_similarity(dataframe,tfidf)\n",
        "\n",
        "\n",
        "cosine_sim_df = cosine_sim_using_sklearn(queries_vector,df_tfidfvect)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 331,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "snaKllK78uBn",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "9a48cdb35a4329d557cb61c11f597021",
          "grade": true,
          "grade_id": "cell-cf95df4104d72ef9",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# tests for cosine similarity\n",
        "assert cosine_sim_df.shape[0] == len(queries)\n",
        "assert cosine_sim_df.shape[1] == len(data)\n",
        "assert (abs(cosine_sim_df)>1).any().any() == False\n",
        "assert np.count_nonzero(cosine_sim_df) == 16\n",
        "assert round(cosine_sim_df.max(),3).max() == 0.888"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LFYms4iqRdY"
      },
      "source": [
        "### BM25\n",
        "In this section we will create a class of Best Match 25 (BM25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP_-2DvoU-mP"
      },
      "source": [
        "To use BM25 we will need to following parameters:\n",
        "\n",
        "* $k1$ and $b$ - free parameters\n",
        "* $f(t_i,D)$ - term frequency of term $t_i$ in document $D$\n",
        "* |$D$|- is the length of the document $D$ in terms \n",
        "* $avgdl$ -  average document length\n",
        "* $IDF$ - which is calculted as follows: $ln(\\frac{(N-n(t_i)+0.5}{n(t_i)+0.5)}+1)$, where $N$ is the total number of documents in the collection, and $n(t_i)$ is the number of documents containing $t_i$ (e.g., document frequency (df))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cmzp5vH1OneY"
      },
      "source": [
        "As a reminder of the data looks like this\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 332,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "p5-0bsOSYbeg",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "394f2ff87725d1f278effd9740e9e64c",
          "grade": false,
          "grade_id": "cell-fabdd53014c09c8c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "4789ca47-30ea-4a63-9139-9ab4185359d3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The sky is blue and we can see the blue sun.',\n",
              " 'The sun is bright and yellow.',\n",
              " 'here comes the blue sun',\n",
              " 'Lucy in the sky with diamonds and you can see the sun in the sky',\n",
              " 'sun sun blue sun here we come',\n",
              " 'Lucy likes blue bright diamonds']"
            ]
          },
          "metadata": {},
          "execution_count": 332
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhVa64REKdHU"
      },
      "source": [
        "**Note:** While using TF-IDF from sklearn we used the `stop_words` argument. \n",
        "\n",
        "**It is not the case when working with BM25 without any package.**\n",
        "Therefore, we need to filter the data and clean it. We will do so utilizing NLTK stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 333,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "V8cC5SngRpGF",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "6e1a182c3e114c3f12885eef14950427",
          "grade": false,
          "grade_id": "cell-4cb3225f5c629f94",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
        "stopwords_frozen = frozenset(stopwords.words('english'))\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    This function aims in tokenize a text into a list of tokens. Moreover, it filter stopwords.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    text: string , represting the text to tokenize.    \n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    list of tokens (e.g., list of tokens).\n",
        "    \"\"\"\n",
        "    list_of_tokens =  [token.group() for token in RE_WORD.finditer(text.lower()) if token.group() not in stopwords_frozen]    \n",
        "    return list_of_tokens\n",
        "\n",
        "\n",
        "clean_data = [tokenize(doc) for doc in data]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHCcrCGYi5b8"
      },
      "source": [
        "Now let's find all the parameters needed for our toy example.\n",
        "**Later on we will need to gather this information from the inverted index.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSSOm4NwBVJT"
      },
      "source": [
        "**YOUR TASK (5 POINTS):**  Complete the implementation of `bm25_preprocess`.\n",
        "This function goes through the data and saves relevant information for the calculation of bm25. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 334,
      "metadata": {
        "deletable": false,
        "id": "Il6NITZLZzfP",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "31098f3ab429c4b6e41eee34c6d2d8c7",
          "grade": false,
          "grade_id": "cell-e004bc9ef56460cf",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def bm25_preprocess(data):\n",
        "    \"\"\"\n",
        "    This function goes through the data and saves relevant information for the calculation of bm25. \n",
        "    Specifically, in this function, we will create 3 objects that gather information regarding document length, term frequency and\n",
        "    document frequency.\n",
        "    Parameters\n",
        "    -----------\n",
        "    data: list of lists. Each inner list is a list of tokens. \n",
        "    Example of data: \n",
        "    [\n",
        "        ['sky', 'blue', 'see', 'blue', 'sun'],\n",
        "        ['sun', 'bright', 'yellow'],\n",
        "        ['comes', 'blue', 'sun'],\n",
        "        ['lucy', 'sky', 'diamonds', 'see', 'sun', 'sky'],\n",
        "        ['sun', 'sun', 'blue', 'sun'],\n",
        "        ['lucy', 'likes', 'blue', 'bright', 'diamonds']\n",
        "    ]\n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    three objects as follows:\n",
        "                a) doc_len: list of integer. Each element represents the length of a document.\n",
        "                b) tf: list of dictionaries. Each dictionary corresponds to a document as follows:\n",
        "                                                                    key: term\n",
        "                                                                    value: normalized term frequency (by the length of document)\n",
        "\n",
        "                                                                                               \n",
        "                c) df: dictionary representing the document frequency as follows:\n",
        "                                                                    key: term\n",
        "                                                                    value: document frequency\n",
        "    \"\"\"   \n",
        "    \n",
        "    # YOUR CODE HERE   \n",
        "    doc_len = []\n",
        "    tf = []\n",
        "    df = {}\n",
        "\n",
        "    for doc in data :\n",
        "      doc_len.append(len(doc))\n",
        "      dictionary = {}\n",
        "      for term in doc :\n",
        "        value = doc.count(term)/doc_len[-1]\n",
        "        dictionary[term] = value\n",
        "      tf.append(dictionary)\n",
        "    \n",
        "    for dic in tf :\n",
        "      for key in dic.keys():\n",
        "        counter = 0\n",
        "        for doc in data :\n",
        "          if doc.count(key)>0 :\n",
        "            counter += 1\n",
        "        df[key] = counter\n",
        "\n",
        "    return doc_len,tf,df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 335,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "LgPAqnBB_9bv",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "372317bedf0270e875b9e1d1ad5f43ca",
          "grade": true,
          "grade_id": "cell-625bfc1263a0f133",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# tests - preprocess data for naive bm25\n",
        "doc_len,tf,df = bm25_preprocess(clean_data)\n",
        "assert df['blue'] == 4\n",
        "assert sum(tf[0].values()) == 1\n",
        "assert sum(doc_len) > sum(df.values())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JmpJk7BC9N1"
      },
      "source": [
        "**YOUR TASK (5 POINTS):** Complete the implementation of `calc_idf` and `_score` at the BM25 class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 336,
      "metadata": {
        "deletable": false,
        "id": "JjQUMtA5dFg4",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4a75d3a1d52e7b235e1ce755de64152e",
          "grade": false,
          "grade_id": "cell-87f2412194ddeedf",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "class BM25:\n",
        "    \"\"\"\n",
        "    Best Match 25.\n",
        "\n",
        "    Parameters to tune\n",
        "    ----------\n",
        "    k1 : float, default 1.5\n",
        "\n",
        "    b : float, default 0.75\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    tf_ : list[dict[str, int]]\n",
        "        Term Frequency per document. So [{'hi': 1}] means\n",
        "        the first document contains the term 'hi' 1 time.\n",
        "        The frequnecy is normilzied by the max term frequency for each document.\n",
        "\n",
        "    doc_len_ : list[int]\n",
        "        Number of terms per document. So [3] means the first\n",
        "        document contains 3 terms. \n",
        "        \n",
        "    df_ : dict[str, int]\n",
        "        Document Frequency per term. i.e. Number of documents in the\n",
        "        corpus that contains the term.       \n",
        "\n",
        "    avg_doc_len_ : float\n",
        "        Average number of terms for documents in the corpus.\n",
        "\n",
        "    idf_ : dict[str, float]\n",
        "        Inverse Document Frequency per term.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,doc_len,df,tf=None,k1=1.5, b=0.75):\n",
        "        self.b = b\n",
        "        self.k1 = k1\n",
        "        self.tf_ = tf\n",
        "        self.doc_len_ = doc_len\n",
        "        self.df_ = df\n",
        "        self.N_ = len(doc_len)\n",
        "        self.avgdl_ = sum(doc_len) / len(doc_len)        \n",
        "        \n",
        "\n",
        "    def calc_idf(self,query):\n",
        "        \"\"\"\n",
        "        This function calculate the idf values according to the BM25 idf formula for each term in the query.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        query: list of token representing the query. For example: ['look', 'blue', 'sky']\n",
        "        \n",
        "        Returns:\n",
        "        -----------\n",
        "        idf: dictionary of idf scores. As follows: \n",
        "                                                    key: term\n",
        "                                                    value: bm25 idf score\n",
        "        \"\"\"\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "\n",
        "        self.idf_ ={}\n",
        "        for term in query:\n",
        "          if term in self.df_.keys():\n",
        "            idf = math.log(( self.N_ + 1) / self.df_[term] )\n",
        "            self.idf_[term] = idf\n",
        "        return self.idf_\n",
        "        \n",
        "\n",
        "    def search(self, queries):\n",
        "        \"\"\"\n",
        "        This function use the _score function to calculate the bm25 score for all queries provided.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        queries: list of lists. Each inner list is a list of tokens. For example:\n",
        "                                                                                    [\n",
        "                                                                                        ['look', 'blue', 'sky'],\n",
        "                                                                                        ['likes', 'blue', 'sun'],\n",
        "                                                                                        ['likes', 'diamonds']\n",
        "                                                                                    ]\n",
        "\n",
        "        Returns:\n",
        "        -----------\n",
        "        list of scores of bm25\n",
        "        \"\"\"\n",
        "        scores = []\n",
        "        for query in queries:            \n",
        "            scores.append([self._score(query, doc_id) for doc_id in range(self.N_)])\n",
        "        return scores\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 337,
      "metadata": {
        "deletable": false,
        "id": "Vxbt-DzED8I3",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "1f019553eb7c74d6babcd04b703835a1",
          "grade": false,
          "grade_id": "cell-3d10bde86f291f80",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "class BM25(BM25):\n",
        "\n",
        "    def _score(self, query, doc_id):\n",
        "        \"\"\"\n",
        "        This function calculate the bm25 score for given query and document.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        query: list of token representing the query. For example: ['look', 'blue', 'sky']\n",
        "        doc_id: integer, document id.\n",
        "        \n",
        "        Returns:\n",
        "        -----------\n",
        "        score: float, bm25 score.\n",
        "        \"\"\"\n",
        "        # YOUR CODE HERE\n",
        "        self.calc_idf(query)\n",
        "        score = 0\n",
        "        for term in query:\n",
        "            cwq =1 \n",
        "            dic = self.tf_[doc_id]\n",
        "            for key in dic :\n",
        "              if key == term :\n",
        "                cwd = dic[key] \n",
        "                up = (self.k1+1)*cwd\n",
        "                down = cwd + self.k1*( 1 - self.b + self.b * (self.doc_len_[doc_id] / self.avgdl_) )\n",
        "                score += cwq * ( up / down ) * self.idf_[term]\n",
        "        return score\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IT49O3A_Peqv"
      },
      "source": [
        "Let's create a new instance of BM25, and calculte its score for all queries. \n",
        "Pay attetnion - we need to tokenize and filter the stopwords from the original queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 338,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "_5gTAhPtjKSO",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "bd2bb445283c5d5b88036ecdbc97b03c",
          "grade": false,
          "grade_id": "cell-109a059a67e0578a",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "bm25 = BM25(tf=tf,doc_len=doc_len,df=df)\n",
        "\n",
        "# Remove tokenizing and remove stopwords from queries\n",
        "clean_queries = [tokenize(query) for query in queries]\n",
        "BM25_res = bm25.search(clean_queries)\n",
        "BM25_df = pd.DataFrame(data = BM25_res,index = [query_id for query_id in range(len(clean_queries))] ,columns = [doc_id for doc_id in range(len(data))])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 339,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "XGatJojrSiFw",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "11a4b850e45b229e195b163bce3b3a07",
          "grade": true,
          "grade_id": "cell-ac576e8f2bdecc09",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# tests BM25\n",
        "assert BM25_df[0][0] == BM25_df[0][2]\n",
        "assert BM25_df[2][0] == BM25_df[2][2]\n",
        "assert BM25_df[3][0] != BM25_df[3][2]\n",
        "assert (BM25_df>1).any().any() == True\n",
        "assert (BM25_df==0).any().any() == True\n",
        "assert BM25_df.sum().argmax() == 5\n",
        "assert BM25_df.sum().argmin() == 1\n",
        "assert sum(bm25.search([['amit','livne'],['lucy','likes','blue','bright','diamonds']])[0]) == 0\n",
        "assert (bm25.search([['amit','livne'],['lucy','likes','blue','bright','diamonds']])[1][-1] > BM25_df[5][2])\n",
        "assert (bm25.search([['amit','livne'],['lucy','likes','blue','bright','diamonds']])[1][2] > BM25_df[1][2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPzSYz85THs2"
      },
      "source": [
        "We can now search for top-N documents for each query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1lkrsKyGVAF"
      },
      "source": [
        "**YOUR TASK (5 POINTS):**  Complete the implementation of `top_N_documents`, which sort and filter the top N docuemnts (by score) for each query.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 340,
      "metadata": {
        "deletable": false,
        "id": "NBsw59bxTHKu",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "36c9728d7dab2d615f2304245f5f25e1",
          "grade": false,
          "grade_id": "cell-1d1a8bc5c9fc90b9",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def top_N_documents(df,N):\n",
        "    \"\"\"\n",
        "    This function sort and filter the top N docuemnts (by score) for each query.\n",
        "    \n",
        "    Parameters\n",
        "    ----------    \n",
        "    df: DataFrame (queries as rows, documents as columns)\n",
        "    N: Integer (how many document to retrieve for each query)    \n",
        "\n",
        "    Returns:\n",
        "    ----------\n",
        "    top_N: dictionary is the following stracture:\n",
        "          key - query id.\n",
        "          value - sorted (according to score) list of pairs lengh of N. Eac pair within the list provide the following information (doc id, score)\n",
        "    \"\"\"    \n",
        "    # YOUR CODE HERE\n",
        "   \n",
        "    dictionary = {}\n",
        "    for q in range(df.shape[0]):\n",
        "      lst = []\n",
        "      for doc_id in range(df.shape[1]):\n",
        "        tuple1 = ( doc_id , df[q][doc_id]) \n",
        "        lst.append( tuple1 )\n",
        "      lst.sort( key = lambda tup: tup[1], reverse = True )\n",
        "      dictionary [q] = lst[:N]\n",
        "    return dictionary  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 341,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "M1vqkZg_QQbR",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "67d2c26dcac53e52c8a3179c8aa149d3",
          "grade": true,
          "grade_id": "cell-49a6ccc6d46bf070",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# test - topN documents\n",
        "top_2_docs = top_N_documents(cosine_sim_df,2)\n",
        "top_10_docs = top_N_documents(cosine_sim_df,10)\n",
        "\n",
        "assert len(top_2_docs[0]) == 2\n",
        "assert len(top_2_docs[0][0]) == 2\n",
        "assert (top_2_docs[0][0][1] > top_N_documents(cosine_sim_df,2)[0][1][1])\n",
        "assert (top_10_docs[0][-1][-1] == 0)\n",
        "assert (top_10_docs[0][-1][-1] != top_10_docs[1][-1][-1])\n",
        "assert (top_10_docs[0][0][-1] != top_10_docs[1][0][-1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcTOOAiTr_0s"
      },
      "source": [
        "The top-N functionality runs on all document and create a large DataFrame. What will happen if the number of documents is much larger? \n",
        "Will this function be efficient? \n",
        "\n",
        "Next, we will build an inverted index (like we did in assignment two). Then, we will use the inverted index utilizing the posting list to create a candidate list of documents per query. \n",
        "Therefore, we will narrow the documents relevant per query.\n",
        "\n",
        "Only then will we calculate the TF-IDF and cosine similarity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4GVs6-Tho-P"
      },
      "source": [
        "## Tasks 2: Implement compound ranking based on title and body using the inverted index (20 points).\n",
        "\n",
        "In order to do so, we will first perform parsing followed by building an Inverted index.\n",
        "Then we will create a searching and ranking mechanism.\n",
        "\n",
        "In this task, you will use Cranfield Corpus. But we need to do some setups in advance. \n",
        "\n",
        "**Setups:**\n",
        "\n",
        "*Make sure you upload the carnfield.py file. attached with this assignment*\n",
        "\n",
        "First, we will load it into the following data structures:\n",
        "\n",
        "* cran_txt_data - list of tuples. An example is provided below. Hold information regards the carnfield dataset.\n",
        "* cran_qry_rel_data - list of tuples. An example is provided below. Holds information regards the queries.\n",
        "\n",
        "We have already done all the load and preprocessing of the data for you.\n",
        "\n",
        "We split the queries data into the training and test sets.\n",
        "Next, we build two inverted indices (one built upon title information and the second on the text\\body of the documents). \n",
        "\n",
        "**After finishing the setups, we implement the ranking mechanism (TF-IDF and BM25 build upon inverted index).**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVFkJuuJnGQQ"
      },
      "source": [
        "### Cranfield Corpus\n",
        "\n",
        "You can get the corpus from [this link](http://ir.dcs.gla.ac.uk/resources/test_collections/cran/).  <br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wox5_m84nKdd"
      },
      "source": [
        "**Parsing**\n",
        "\n",
        "For detailed information about the parsing of this corpus look at [ this Notebook](https://colab.research.google.com/github/pragmalingu/experiments/blob/master/00_Data/CranfieldCorpus.ipynb) or, for parsing in general, read [this guide](https://pragmalingu.de/docs/guides/how-to-parse). An overview of the format of the files can be found here: [Data Sets](https://pragmalingu.de/docs/guides/data-comparison)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 342,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "dV4349DnFaXB",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "09c99b9d9f27ed919624684965d896dd",
          "grade": false,
          "grade_id": "cell-c80adb0d51ad7c13",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "import carnfield\n",
        "\n",
        "cran_txt_data,cran_qry_rel_data = carnfield.main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stmEFWtVAXlb"
      },
      "source": [
        "### Data seperation\n",
        "In real-world scenarios, we do not know all the queries in advance. Thus, we are familiar with some queries and can use them to tune the parameters of our model (e.g., the k and b for BM25 or term that appears at least n times within the data, etc.'). We will split the queries into two sets to simulate this scenario: Train and Test.\n",
        "\n",
        "You can and should use the data from the train set to tune the parameters of your retrieval model. **You should not use the test data for tuning.**\n",
        "\n",
        "\n",
        "The test data should be used to evaluate your model according to the metrics in the following section (e.g., precision, recall etc.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk9882itsCaA"
      },
      "source": [
        "Example of cran_txt_data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 343,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "xre0ixPVrqqg",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "9ba9aeca8896a712ab5b86db33576d44",
          "grade": false,
          "grade_id": "cell-f40c98bca2f5746c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebc1637a-3ea3-44e4-ac5e-bfae38c05e2f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('1',\n",
              "  {'author': ' brenckman,m. ',\n",
              "   'publication_date': ' j. ae. scs. 25, 1958, 324. ',\n",
              "   'text': '  an experimental study of a wing in a propeller slipstream was made in order to determine the spanwise distribution of the lift increase due to slipstream at different angles of attack of the wing and at different free stream to slipstream velocity ratios .  the results were intended in part as an evaluation basis for different theoretical treatments of this problem .   the comparative span loading curves, together with supporting evidence, showed that a substantial part of the lift increment produced by the slipstream was due to a /destalling/ or boundary-layer-control effect .  the integrated remaining lift increment, after subtracting this destalling lift, was found to agree well with a potential flow theory .   an empirical evaluation of the destalling effects was made for the specific configuration of the experiment . ',\n",
              "   'title': ' experimental investigation of the aerodynamics of a wing in a slipstream . '})]"
            ]
          },
          "metadata": {},
          "execution_count": 343
        }
      ],
      "source": [
        " list(islice(cran_txt_data.items(), 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfP-ttKPsGVT"
      },
      "source": [
        "Example of cran_qry_rel_data. \n",
        "For each query id you can see the question and the relevance assessments is a list of tuples of relevant document. The tuple structure is as follows: (document id, relevance score). Therefore a tuple such as (184,2) indicates that the document id 184 relevance score is 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 344,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "UUGuJu8ir0YZ",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5e22d13c06e697a7abec2d9b6fb12ab0",
          "grade": false,
          "grade_id": "cell-5d61afe05409a797",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e27ee80-3766-491f-a6cb-712e62c58a59"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1,\n",
              "  {'question': ' what similarity laws must be obeyed when constructing aeroelastic models of heated high speed aircraft . ',\n",
              "   'relevance_assessments': [(184, 2),\n",
              "    (29, 2),\n",
              "    (31, 2),\n",
              "    (12, 3),\n",
              "    (51, 3),\n",
              "    (102, 3),\n",
              "    (13, 4),\n",
              "    (14, 4),\n",
              "    (15, 4),\n",
              "    (57, 2),\n",
              "    (378, 2),\n",
              "    (859, 2),\n",
              "    (185, 3),\n",
              "    (30, 3),\n",
              "    (37, 3),\n",
              "    (52, 4),\n",
              "    (142, 4),\n",
              "    (195, 4),\n",
              "    (875, 2),\n",
              "    (56, 3),\n",
              "    (66, 3),\n",
              "    (95, 3),\n",
              "    (462, 4),\n",
              "    (497, 3),\n",
              "    (858, 3),\n",
              "    (876, 3),\n",
              "    (879, 3),\n",
              "    (880, 3),\n",
              "    (486, 1)]})]"
            ]
          },
          "metadata": {},
          "execution_count": 344
        }
      ],
      "source": [
        " list(islice(cran_qry_rel_data.items(), 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tftcouOuD4cS"
      },
      "source": [
        "Split to train and test queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 345,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "fC0sqfVLAxwB",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5ac5ee4de3b77664f1993c350e6f4a52",
          "grade": false,
          "grade_id": "cell-3b3f8520734a9bbf",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "cran_qry_rel_data_train = list(islice(cran_qry_rel_data.items(), 0,180))\n",
        "cran_qry_rel_data_test = list(islice(cran_qry_rel_data.items(), 180,225))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMFJey2QXWvp"
      },
      "source": [
        "Next, we will use the inverted index from assignment 2 (with some minor adaptations).\n",
        "We save a dictionary named `DL`, which fetches the document length of each document.\n",
        "\n",
        "Upon using the index functionality of add_doc, `DL` needs to be updated as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qf028y32uxYx"
      },
      "source": [
        "### Inverted index (code from assignment 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_gmmMRIuzlR"
      },
      "source": [
        "#### Helper classes (code from assignment 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 346,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "Z4wUkB8HuIfL",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3fc27cc23ffcfb265d5ac128f4c53a94",
          "grade": false,
          "grade_id": "cell-8635d11f012f2063",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# Let's start with a small block size of 30 bytes just to test things out. \n",
        "BLOCK_SIZE = 199998\n",
        "\n",
        "class MultiFileWriter:\n",
        "    \"\"\" Sequential binary writer to multiple files of up to BLOCK_SIZE each. \"\"\"\n",
        "    def __init__(self, base_dir, name):\n",
        "        self._base_dir = Path(base_dir)\n",
        "        self._name = name\n",
        "        self._file_gen = (open(self._base_dir / f'{name}_{i:03}.bin', 'wb') \n",
        "                          for i in count())\n",
        "        self._f = next(self._file_gen)\n",
        "    \n",
        "    def write(self, b):\n",
        "      locs = []\n",
        "      while len(b) > 0:\n",
        "        pos = self._f.tell()\n",
        "        remaining = BLOCK_SIZE - pos\n",
        "        # if the current file is full, close and open a new one.\n",
        "        if remaining == 0:  \n",
        "          self._f.close()\n",
        "          self._f = next(self._file_gen)\n",
        "          pos, remaining = 0, BLOCK_SIZE\n",
        "        self._f.write(b[:remaining])\n",
        "        locs.append((self._f.name, pos))\n",
        "        b = b[remaining:]\n",
        "      return locs\n",
        "\n",
        "    def close(self):\n",
        "      self._f.close()\n",
        "\n",
        "class MultiFileReader:\n",
        "  \"\"\" Sequential binary reader of multiple files of up to BLOCK_SIZE each. \"\"\"\n",
        "  def __init__(self):\n",
        "    self._open_files = {}\n",
        "\n",
        "  def read(self, locs, n_bytes):\n",
        "    b = []\n",
        "    for f_name, offset in locs:\n",
        "      if f_name not in self._open_files:\n",
        "        self._open_files[f_name] = open(f_name, 'rb')\n",
        "      f = self._open_files[f_name]\n",
        "      f.seek(offset)\n",
        "      n_read = min(n_bytes, BLOCK_SIZE - offset)\n",
        "      b.append(f.read(n_read))\n",
        "      n_bytes -= n_read\n",
        "    return b''.join(b)\n",
        "  \n",
        "  def close(self):\n",
        "    for f in self._open_files.values():\n",
        "      f.close()\n",
        "\n",
        "  def __exit__(self, exc_type, exc_value, traceback):\n",
        "    self.close()\n",
        "    return False "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 347,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "NoJlbdnHu2MR",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4739ecbbbb2ea8cb87a1d652ffb28916",
          "grade": false,
          "grade_id": "cell-a9aff035d9c869a8",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "TUPLE_SIZE = 6       # We're going to pack the doc_id and tf values in this \n",
        "                     # many bytes.\n",
        "TF_MASK = 2 ** 16 - 1 # Masking the 16 low bits of an integer\n",
        "\n",
        "DL = {}  # We're going to update and calculate this after each document. This will be usefull for the calculation of AVGDL (utilized in BM25) \n",
        "class InvertedIndex:  \n",
        "  def __init__(self, docs={}):\n",
        "    \"\"\" Initializes the inverted index and add documents to it (if provided).\n",
        "    Parameters:\n",
        "    -----------\n",
        "      docs: dict mapping doc_id to list of tokens\n",
        "    \"\"\"\n",
        "    # stores document frequency per term\n",
        "    self.df = Counter()\n",
        "    # stores total frequency per term\n",
        "    self.term_total = Counter()\n",
        "    # stores posting list per term while building the index (internally), \n",
        "    # otherwise too big to store in memory.\n",
        "    self._posting_list = defaultdict(list)\n",
        "    # mapping a term to posting file locations, which is a list of \n",
        "    # (file_name, offset) pairs. Since posting lists are big we are going to\n",
        "    # write them to disk and just save their location in this list. We are \n",
        "    # using the MultiFileWriter helper class to write fixed-size files and store\n",
        "    # for each term/posting list its list of locations. The offset represents \n",
        "    # the number of bytes from the beginning of the file where the posting list\n",
        "    # starts. \n",
        "    self.posting_locs = defaultdict(list)\n",
        "    \n",
        "    for doc_id, tokens in docs.items():\n",
        "      self.add_doc(doc_id, tokens)\n",
        "\n",
        "  def add_doc(self, doc_id, tokens):\n",
        "    \"\"\" Adds a document to the index with a given `doc_id` and tokens. It counts\n",
        "        the tf of tokens, then update the index (in memory, no storage \n",
        "        side-effects).\n",
        "    \"\"\"\n",
        "    DL[(doc_id)] = DL.get(doc_id,0) + (len(tokens))\n",
        "    w2cnt = Counter(tokens)\n",
        "    self.term_total.update(w2cnt)\n",
        "    max_value = max(w2cnt.items(), key=operator.itemgetter(1))[1]    \n",
        "    # frequencies = {key: value/max_value for key, value in frequencies.items()}\n",
        "    for w, cnt in w2cnt.items():        \n",
        "        self.df[w] = self.df.get(w, 0) + 1                \n",
        "        self._posting_list[w].append((doc_id, cnt))\n",
        "\n",
        "\n",
        "  def write(self, base_dir, name):\n",
        "    \"\"\" Write the in-memory index to disk and populate the `posting_locs`\n",
        "        variables with information about file location and offset of posting\n",
        "        lists. Results in at least two files: \n",
        "        (1) posting files `name`XXX.bin containing the posting lists.\n",
        "        (2) `name`.pkl containing the global term stats (e.g. df).\n",
        "    \"\"\"\n",
        "    #### POSTINGS ####\n",
        "    self.posting_locs = defaultdict(list)\n",
        "    with closing(MultiFileWriter(base_dir, name)) as writer:\n",
        "      # iterate over posting lists in lexicographic order\n",
        "      for w in sorted(self._posting_list.keys()):\n",
        "        self._write_a_posting_list(w, writer, sort=True)\n",
        "    #### GLOBAL DICTIONARIES ####\n",
        "    self._write_globals(base_dir, name)\n",
        "\n",
        "  def _write_globals(self, base_dir, name):\n",
        "    with open(Path(base_dir) / f'{name}.pkl', 'wb') as f:\n",
        "      pickle.dump(self, f)\n",
        "\n",
        "  def _write_a_posting_list(self, w, writer, sort=False):\n",
        "    # sort the posting list by doc_id\n",
        "    pl = self._posting_list[w]\n",
        "    if sort:\n",
        "      pl = sorted(pl, key=itemgetter(0))\n",
        "    # convert to bytes    \n",
        "    b = b''.join([(int(doc_id) << 16 | (tf & TF_MASK)).to_bytes(TUPLE_SIZE, 'big')\n",
        "                  for doc_id, tf in pl])\n",
        "    # write to file(s)\n",
        "    locs = writer.write(b)\n",
        "    # save file locations to index\n",
        "    self.posting_locs[w].extend(locs) \n",
        "\n",
        "  def __getstate__(self):\n",
        "    \"\"\" Modify how the object is pickled by removing the internal posting lists\n",
        "        from the object's state dictionary. \n",
        "    \"\"\"\n",
        "    state = self.__dict__.copy()\n",
        "    del state['_posting_list']\n",
        "    return state\n",
        "\n",
        "  @staticmethod\n",
        "  def read_index(base_dir, name):\n",
        "    with open(Path(base_dir) / f'{name}.pkl', 'rb') as f:\n",
        "      return pickle.load(f)\n",
        "\n",
        "  @staticmethod\n",
        "  def delete_index(base_dir, name):\n",
        "    path_globals = Path(base_dir) / f'{name}.pkl'\n",
        "    path_globals.unlink()\n",
        "    for p in Path(base_dir).rglob(f'{name}_*.bin'):\n",
        "      p.unlink()\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHHVIZKaIhWP"
      },
      "source": [
        "In this assignment we will generate two indexes. \n",
        "One for titles and one for body (e.g., text).\n",
        "First, we need to tokenize the title and body textual information (e.g., text). \n",
        "Note: it can take several minutes to tokenize it.\n",
        "\n",
        "Next, we will create two seperated indexes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 348,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "eAweGxojIdFS",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f84722978691688a1a5a50f542b99701",
          "grade": false,
          "grade_id": "cell-8dcd5839ea75616c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "cran_txt_data_titles = {k: tokenize(v['title']) for k,v in cran_txt_data.items()}\n",
        "cran_txt_data_text = {k: tokenize(v['text']) for k,v in cran_txt_data.items()}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8INaoSJEOD5"
      },
      "source": [
        "Next, we need to preprocess the text of the queries for both training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 349,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "MszY8ujbr58s",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f51de33ee19a375ba3a1e9ac13456295",
          "grade": false,
          "grade_id": "cell-d93bc0367a0cb679",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "cran_txt_query_text_train = {q[0]: tokenize(q[1]['question']) for q in cran_qry_rel_data_train}\n",
        "cran_txt_query_text_test = {q[0]: tokenize(q[1]['question']) for q in cran_qry_rel_data_test}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whsU_r5bG8g9"
      },
      "source": [
        "#### Creating and writing the index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 350,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "aEOZhr7QvOMC",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "51092cab286197f0b64e70f927674685",
          "grade": false,
          "grade_id": "cell-ced0648af9ff386f",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "index_titles = InvertedIndex(docs=cran_txt_data_titles)\n",
        "index_text = InvertedIndex(docs=cran_txt_data_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 351,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "Xt5ov8x5Bw8l",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3894e1aef7fee5be7ceec6286605c5e6",
          "grade": false,
          "grade_id": "cell-c2ab9866d400850c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dbad444-f1e7-421b-ed67-512c8f040c5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory â€˜body_indexâ€™: File exists\n",
            "mkdir: cannot create directory â€˜title_indexâ€™: File exists\n"
          ]
        }
      ],
      "source": [
        "# create directories for the different indices \n",
        "!mkdir body_index title_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 352,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "GJdYcsDvxVQ5",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d7f3fbc4afd8d400441bc0637ee54c53",
          "grade": false,
          "grade_id": "cell-9702b30a844d04d4",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "index_titles.write('title_index','title')\n",
        "index_text.write('body_index','body')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFGBVH0uPISq"
      },
      "source": [
        "### Reading data from posting (code taken from assignment 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 353,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "xhMhnuBDQXRz",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "794d472559a667e81c31bdf8bd5d4c20",
          "grade": false,
          "grade_id": "cell-966dec712a0df1a0",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "class InvertedIndex(InvertedIndex):\n",
        "  \n",
        "  def posting_lists_iter(self):\n",
        "    \"\"\" A generator that reads one posting list from disk and yields \n",
        "        a (word:str, [(doc_id:int, tf:int), ...]) tuple.\n",
        "    \"\"\"\n",
        "    with closing(MultiFileReader()) as reader:\n",
        "      for w, locs in self.posting_locs.items():\n",
        "        # read a certain number of bytes into variable b\n",
        "        b = reader.read(locs, self.df[w] * TUPLE_SIZE)\n",
        "        posting_list = []\n",
        "        # convert the bytes read into `b` to a proper posting list.\n",
        "        \n",
        "        for i in range(self.df[w]):\n",
        "          doc_id = int.from_bytes(b[i*TUPLE_SIZE:i*TUPLE_SIZE+4], 'big')\n",
        "          tf = int.from_bytes(b[i*TUPLE_SIZE+4:(i+1)*TUPLE_SIZE], 'big')\n",
        "          posting_list.append((doc_id, tf))\n",
        "        \n",
        "        yield w, posting_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 354,
      "metadata": {
        "id": "vjER_w0oHVXF"
      },
      "outputs": [],
      "source": [
        "def get_posting_gen(index):\n",
        "    \"\"\"\n",
        "    This function returning the generator working with posting list.\n",
        "    \n",
        "    Parameters:\n",
        "    ----------\n",
        "    index: inverted index    \n",
        "    \"\"\"\n",
        "    words, pls = zip(*index.posting_lists_iter())\n",
        "    return words,pls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROqsUP75WtPK"
      },
      "source": [
        "**Example of use**\n",
        "\n",
        "First, we will load the title index. \n",
        "\n",
        "Next, we will search for a given term in both lists (words, and pls)\n",
        "\n",
        "Remineder: pls is the information from the posting list.\n",
        "Each value within the `words` list has a corresponding value in the `pls` list.\n",
        "The value within a `pls` list is a list of tuples. Each tuple in the following format (x,y). Where x represent the document id, and `y` represent the occurence of the term in the document. \n",
        "\n",
        "In this example we are seeking the first term in the `words` list, and we can observe the is appears in 4 different documents. \n",
        "Moreover, we can observe that in document 807 it appears twice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 355,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "5OjIGqGrRX87",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "8ea8ff4fe79a91bbd45fd2f2b05f9ee2",
          "grade": false,
          "grade_id": "cell-c84449616cf446a5",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "idx_title = InvertedIndex.read_index('title_index', 'title')\n",
        "idx_body = InvertedIndex.read_index('body_index', 'body')\n",
        "# read posting lists from disk\n",
        "words, pls = zip(*idx_title.posting_lists_iter())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue6YmSy_yJoN"
      },
      "source": [
        "Let's check for example the document number `807`. \n",
        "We needed its document length (DL) which is suppose to be the length of its title + the length of the text of the document. \n",
        "Let's verify it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 356,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "zoH6o6VpSPg6",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "877ef0593ad9d7d45271dd0271aec210",
          "grade": false,
          "grade_id": "cell-13071edfd63d0501",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0340de17-9da3-4ffa-9200-ef39ed73f2e3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ground',\n",
              " 'measurements',\n",
              " 'shock',\n",
              " 'wave',\n",
              " 'noise',\n",
              " 'supersonic',\n",
              " 'bomber',\n",
              " 'airplanes',\n",
              " 'altitude',\n",
              " 'range',\n",
              " '000',\n",
              " '000',\n",
              " 'feet']"
            ]
          },
          "metadata": {},
          "execution_count": 356
        }
      ],
      "source": [
        "#Let's check the title of document 807\n",
        "cran_txt_data_titles['807']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PizC-65xpzvk"
      },
      "source": [
        "Clearly `000` should not appear twice.\n",
        "Let have a second look on the original document and understand weather it's a mistake within a tokenization step or in the original title."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 357,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "KDFB8nfsp-x8",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f19a50fee8d7c92e03f58038b58e357d",
          "grade": false,
          "grade_id": "cell-0c38f8f79da89633",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "65c6be87-1546-4372-c0f5-32b4a19c020a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' ground measurements of the shock wave noise from supersonic bomber airplanes in the altitude range from 30,000 to 50,000 feet . '"
            ]
          },
          "metadata": {},
          "execution_count": 357
        }
      ],
      "source": [
        "cran_txt_data['807']['title']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wv1fmK21qc2c"
      },
      "source": [
        "As we could have guessed, the token `000` does not appear twice in this document.\n",
        "\n",
        "**Reminder: garbage in = garbage out.**\n",
        "\n",
        "**For this assignment, it is ok to leave it as is. However, verify this part when you are working on your project.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 358,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "NavtokcsyJDs",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "47f0fa32c38a114f89f254f1f02eb3a8",
          "grade": false,
          "grade_id": "cell-c6e509903a909f79",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "assert DL['807'] == len(cran_txt_data_titles['807']) + len(cran_txt_data_text['807'])\n",
        "assert len(DL) == len(cran_txt_data_titles)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCgSReAUwG-M"
      },
      "source": [
        "### Ranking\n",
        "In this section, you will be given a query or queries and return a ranked list of documents to retrieve.\n",
        "In this assignment, we are experimenting with two methods. TF-IDF and BM25. \n",
        "\n",
        "**We will use the inverted index in order to do so and will not utilize the whole corpus in advance.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBlLqQii5f6B"
      },
      "source": [
        "#### TF-IDF for carnfield data (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1qulheONwg8"
      },
      "source": [
        "Bellow cells contain the following functions: \n",
        "\n",
        "*   *generate_query_tfidf_vector* - Generate a vector representing the query\n",
        "*   *get_candidate_documents_and_scores* - Generate a dictionary representing a pool of candidate documents for a given query.\n",
        "*   *generate_document_tfidf_matrix* - Generate a DataFrame of tfidf scores for a given query.\n",
        "*   *cosine_similarity* - Calculate the cosine similarity for each candidate document in D and a given query (e.g., Q). **You will impelement this function (5 points)**\n",
        "\n",
        "*   *get_top_n* - Sort and return the highest N documents according to the cosine similarity score.\n",
        "\n",
        "*   *get_topN_score_for_queries* - Generate a dictionary that gather for every query its topN score. **You will impelement this function (5 points)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 359,
      "metadata": {
        "id": "mbPJ5YxcXRcT"
      },
      "outputs": [],
      "source": [
        "def generate_query_tfidf_vector(query_to_search,index):\n",
        "    \"\"\" \n",
        "    Generate a vector representing the query. Each entry within this vector represents a tfidf score.\n",
        "    The terms representing the query will be the unique terms in the index.\n",
        "\n",
        "    We will use tfidf on the query as well. \n",
        "    For calculation of IDF, use log with base 10.\n",
        "    tf will be normalized based on the length of the query.    \n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    query_to_search: list of tokens (str). This list will be preprocessed in advance (e.g., lower case, filtering stopwords, etc.'). \n",
        "                     Example: 'Hello, I love information retrival' --->  ['hello','love','information','retrieval']\n",
        "\n",
        "    index:           inverted index loaded from the corresponding files.\n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    vectorized query with tfidf scores\n",
        "    \"\"\"\n",
        "    \n",
        "    epsilon = .0000001\n",
        "    total_vocab_size = len(index.term_total)\n",
        "    Q = np.zeros((total_vocab_size))\n",
        "    term_vector = list(index.term_total.keys())    \n",
        "    counter = Counter(query_to_search)\n",
        "    for token in np.unique(query_to_search):\n",
        "        if token in index.term_total.keys(): #avoid terms that do not appear in the index.               \n",
        "            tf = counter[token]/len(query_to_search) # term frequency divded by the length of the query\n",
        "            df = index.df[token]            \n",
        "            idf = math.log((len(DL))/(df+epsilon),10) #smoothing\n",
        "            \n",
        "            try:\n",
        "                ind = term_vector.index(token)\n",
        "                Q[ind] = tf*idf                    \n",
        "            except:\n",
        "                pass\n",
        "    return Q\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 360,
      "metadata": {
        "id": "HhQRw9ye10r1"
      },
      "outputs": [],
      "source": [
        "def get_candidate_documents_and_scores(query_to_search,index,words,pls):\n",
        "    \"\"\"\n",
        "    Generate a dictionary representing a pool of candidate documents for a given query. This function will go through every token in query_to_search\n",
        "    and fetch the corresponding information (e.g., term frequency, document frequency, etc.') needed to calculate TF-IDF from the posting list.\n",
        "    Then it will populate the dictionary 'candidates.'\n",
        "    For calculation of IDF, use log with base 10.\n",
        "    tf will be normalized based on the length of the document.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    query_to_search: list of tokens (str). This list will be preprocessed in advance (e.g., lower case, filtering stopwords, etc.'). \n",
        "                     Example: 'Hello, I love information retrival' --->  ['hello','love','information','retrieval']\n",
        "\n",
        "    index:           inverted index loaded from the corresponding files.\n",
        "\n",
        "    words,pls: generator for working with posting.\n",
        "    Returns:\n",
        "    -----------\n",
        "    dictionary of candidates. In the following format:\n",
        "                                                               key: pair (doc_id,term)\n",
        "                                                               value: tfidf score. \n",
        "    \"\"\"\n",
        "    candidates = {}\n",
        "    N = len(DL)        \n",
        "    for term in np.unique(query_to_search):        \n",
        "        if term in words:            \n",
        "            list_of_doc = pls[words.index(term)]                        \n",
        "            normlized_tfidf = [(doc_id,(freq/DL[str(doc_id)])*math.log(N/index.df[term],10)) for doc_id, freq in list_of_doc]           \n",
        "                        \n",
        "            for doc_id, tfidf in normlized_tfidf:\n",
        "                candidates[(doc_id,term)] = candidates.get((doc_id,term), 0) + tfidf               \n",
        "        \n",
        "    return candidates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 361,
      "metadata": {
        "id": "11pKF-MqFhAt"
      },
      "outputs": [],
      "source": [
        "def generate_document_tfidf_matrix(query_to_search,index,words,pls):\n",
        "    \"\"\"\n",
        "    Generate a DataFrame `D` of tfidf scores for a given query. \n",
        "    Rows will be the documents candidates for a given query\n",
        "    Columns will be the unique terms in the index.\n",
        "    The value for a given document and term will be its tfidf score.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    query_to_search: list of tokens (str). This list will be preprocessed in advance (e.g., lower case, filtering stopwords, etc.'). \n",
        "                     Example: 'Hello, I love information retrival' --->  ['hello','love','information','retrieval']\n",
        "\n",
        "    index:           inverted index loaded from the corresponding files.\n",
        "\n",
        "    words,pls: generator for working with posting.\n",
        "    Returns:\n",
        "    -----------\n",
        "    DataFrame of tfidf scores.\n",
        "    \"\"\"\n",
        "    \n",
        "    total_vocab_size = len(index.term_total)\n",
        "    candidates_scores = get_candidate_documents_and_scores(query_to_search,index,words,pls) #We do not need to utilize all document. Only the docuemnts which have corrspoinding terms with the query.\n",
        "    unique_candidates = np.unique([doc_id for doc_id, freq in candidates_scores.keys()])\n",
        "    D = np.zeros((len(unique_candidates), total_vocab_size))\n",
        "    D = pd.DataFrame(D)\n",
        "    \n",
        "    D.index = unique_candidates\n",
        "    D.columns = index.term_total.keys()\n",
        "\n",
        "    for key in candidates_scores:\n",
        "        tfidf = candidates_scores[key]\n",
        "        doc_id, term = key    \n",
        "        D.loc[doc_id][term] = tfidf\n",
        "\n",
        "    return D\n",
        "     \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yk_yJnzM5Tb"
      },
      "source": [
        "**YOUR TASK (5 POINTS):** Complete the implementation of `cosine_similarity`. This function calculate the cosine similarity for each candidate document in D and a given query (e.g., Q) and return a dictionary of cosine similary scores.\n",
        "\n",
        "**Note:** for this task you cannot use sklearn. However, you can use pandas or numpy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 362,
      "metadata": {
        "deletable": false,
        "id": "92OO-c4Ah7TJ",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3334e351e5f2e85d04687f6640f4aa37",
          "grade": false,
          "grade_id": "cell-fab214a9b44a8c5e",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(D,Q):\n",
        "    \"\"\"\n",
        "    Calculate the cosine similarity for each candidate document in D and a given query (e.g., Q).\n",
        "    Generate a dictionary of cosine similarity scores \n",
        "    key: doc_id\n",
        "    value: cosine similarity score\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    D: DataFrame of tfidf scores.\n",
        "\n",
        "    Q: vectorized query with tfidf scores\n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    dictionary of cosine similarity score as follows:\n",
        "                                                                key: document id (e.g., doc_id)\n",
        "                                                                value: cosine similarty score.\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    dictionary = {}\n",
        "    for doc_id,row in D.iterrows():\n",
        "      d = row\n",
        "      score = dot(d, Q)/(norm(d)*norm(Q))\n",
        "      dictionary[doc_id] = score\n",
        "    return dictionary\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 363,
      "metadata": {
        "id": "Yh7H2unw9oTf"
      },
      "outputs": [],
      "source": [
        "def get_top_n(sim_dict,N=3):\n",
        "    \"\"\" \n",
        "    Sort and return the highest N documents according to the cosine similarity score.\n",
        "    Generate a dictionary of cosine similarity scores \n",
        "   \n",
        "    Parameters:\n",
        "    -----------\n",
        "    sim_dict: a dictionary of similarity score as follows:\n",
        "                                                                key: document id (e.g., doc_id)\n",
        "                                                                value: similarity score. We keep up to 5 digits after the decimal point. (e.g., round(score,5))\n",
        "\n",
        "    N: Integer (how many documents to retrieve). By default N = 3\n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    a ranked list of pairs (doc_id, score) in the length of N.\n",
        "    \"\"\"\n",
        "    \n",
        "    return sorted([(doc_id,round(score,5)) for doc_id, score in sim_dict.items()], key = lambda x: x[1],reverse=True)[:N]\n",
        "    \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziJUOZwrMTiL"
      },
      "source": [
        "**YOUR TASK (5 POINTS)**: Complete the implementation of `get_topN_score_for_queries`. This function generate a dictionary that gather for every query its topN score, based on cosine similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 364,
      "metadata": {
        "deletable": false,
        "id": "2AJ1qn2YVpN5",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "14f6615a5239e4111808f84386c96e0a",
          "grade": false,
          "grade_id": "cell-3d601abdd80ad4b0",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def get_topN_score_for_queries(queries_to_search,index,N=3):\n",
        "    \"\"\" \n",
        "    Generate a dictionary that gathers for every query its topN score.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    queries_to_search: a dictionary of queries as follows: \n",
        "                                                        key: query_id\n",
        "                                                        value: list of tokens.\n",
        "    index:           inverted index loaded from the corresponding files.    \n",
        "    N: Integer. How many documents to retrieve. This argument is passed to the topN function. By default N = 3, for the topN function. \n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    return: a dictionary of queries and topN pairs as follows:\n",
        "                                                        key: query_id\n",
        "                                                        value: list of pairs in the following format:(doc_id, score). \n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    dictionary = {}\n",
        "    for query_id in range(1,len(queries_to_search.keys())+1) :\n",
        "      Q = generate_query_tfidf_vector(queries_to_search[query_id],index)\n",
        "      D = generate_document_tfidf_matrix(queries_to_search[query_id],index,words,pls)\n",
        "      dic_score = cosine_similarity(D,Q)\n",
        "      dictionary[query_id] = get_top_n(dic_score,N=3) \n",
        "    return dictionary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 365,
      "metadata": {
        "id": "oxYOsSwMWDmo"
      },
      "outputs": [],
      "source": [
        "tfidf_queries_score_train = get_topN_score_for_queries(cran_txt_query_text_train,idx_title)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 366,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "F_fNI7HWQaSm",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "912cbb469617531fcbb09cbfb7052046",
          "grade": true,
          "grade_id": "cell-71abe20e2c2a59a7",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "#tests \n",
        "\n",
        "assert len(tfidf_queries_score_train)==180\n",
        "assert 0 not in tfidf_queries_score_train.keys()\n",
        "assert len(tfidf_queries_score_train[5])==3\n",
        "assert tfidf_queries_score_train[172][0][1] == tfidf_queries_score_train[172][1][1]\n",
        "assert tfidf_queries_score_train[14][0][1] == tfidf_queries_score_train[172][1][1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox7l1WIMatYa"
      },
      "source": [
        "For query 172 we can observe two document with cosine similarity score of 1. Let's have a glance on this query and documents for making sure it makes sense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 367,
      "metadata": {
        "id": "6yB6gHojaN77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "517cf3c3-79fc-4263-806d-04f6a1c61da5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "relevnt documents and tfidf score for query number 172 : [(320, 1.0), (321, 1.0), (322, 1.0)]\n",
            "query:  ['solution', 'blasius', 'problem', 'three-point', 'boundary', 'conditions']\n",
            "docuemnt 320:  ['comment', 'improved', 'numerical', 'solution', 'blasius', 'problem', 'three-point', 'boundary', 'conditions']\n",
            "docuemnt 320:  ['improved', 'numerical', 'solution', 'blasius', 'problem', 'three-point', 'boundary', 'conditions']\n",
            "docuemnt 322:  ['numerical', 'solution', 'blasius', 'problem', 'three-point', 'boundary', 'conditions']\n"
          ]
        }
      ],
      "source": [
        "print('relevnt documents and tfidf score for query number 172 :',tfidf_queries_score_train[172])\n",
        "print('query: ' ,cran_txt_query_text_train[172])\n",
        "print('docuemnt 320: ', cran_txt_data_titles['320'])\n",
        "print('docuemnt 320: ', cran_txt_data_titles['321'])\n",
        "print('docuemnt 322: ', cran_txt_data_titles['322'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swC-nUdwviXN"
      },
      "source": [
        "#### BM25 for carnfield data (10 points)\n",
        "As a reminder:\n",
        "\n",
        "To use BM25 we will need to following parameters:\n",
        "\n",
        "* $k1$ and $b$ - free parameters\n",
        "* $f(t_i,D)$ - term frequency of term $t_i$ in document $D$\n",
        "* |$D$|- is the length of the document $D$ in terms \n",
        "* $avgdl$ -  average document length\n",
        "* $IDF$ - which is calculted as follows: $ln(\\frac{(N-n(t_i)+0.5}{n(t_i)+0.5)}+1)$, where $N$ is the total number of documents in the collection, and $n(t_i)$ is the number of documents containing $t_i$ (e.g., document frequency (df)).\n",
        "\n",
        "Now, we will use the inverted index to fetch this information.\n",
        "\n",
        "**We need to check only documents which are 'candidates' for a given query.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rk090Q-59Jov"
      },
      "source": [
        "We can create a simpler version of get candidate functions. (We do not need to calculate the TFIDF scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 368,
      "metadata": {
        "id": "HQKCt86l3_ml"
      },
      "outputs": [],
      "source": [
        "def get_candidate_documents(query_to_search,index,words,pls):\n",
        "    \"\"\"\n",
        "    Generate a dictionary representing a pool of candidate documents for a given query. \n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    query_to_search: list of tokens (str). This list will be preprocessed in advance (e.g., lower case, filtering stopwords, etc.'). \n",
        "                     Example: 'Hello, I love information retrival' --->  ['hello','love','information','retrieval']\n",
        "\n",
        "    index:           inverted index loaded from the corresponding files.\n",
        "\n",
        "    words,pls: generator for working with posting.\n",
        "    Returns:\n",
        "    -----------\n",
        "    list of candidates. In the following format:\n",
        "                                                               key: pair (doc_id,term)\n",
        "                                                               value: tfidf score. \n",
        "    \"\"\"\n",
        "    candidates = []    \n",
        "    for term in np.unique(query_to_search):\n",
        "        if term in words:        \n",
        "            current_list = (pls[words.index(term)])                \n",
        "            candidates += current_list    \n",
        "    return np.unique(candidates)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqf4UeRPTUQm"
      },
      "source": [
        "**YOUR TASK (10 POINTS):** Complete the implementation of `search` at the BM25 class.\n",
        "Differently, from previous implememntation, this time you should check only documents which are 'candidates' for a given query. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 369,
      "metadata": {
        "deletable": false,
        "id": "nPOS5HoohGTJ",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "1866e668d1c0d235e49d047c4ff954ad",
          "grade": false,
          "grade_id": "cell-bb4b866e29cf18ab",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from itertools import chain\n",
        "import time\n",
        "# When preprocessing the data have a dictionary of document length for each document saved in a variable called `DL`.\n",
        "class BM25_from_index:\n",
        "    \"\"\"\n",
        "    Best Match 25.    \n",
        "    ----------\n",
        "    k1 : float, default 1.5\n",
        "\n",
        "    b : float, default 0.75\n",
        "\n",
        "    index: inverted index\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,index,k1=1.5, b=0.75):\n",
        "        self.b = b\n",
        "        self.k1 = k1\n",
        "        self.index = index\n",
        "        self.N = len(DL)\n",
        "        self.AVGDL = sum(DL.values())/self.N\n",
        "        self.words, self.pls = zip(*self.index.posting_lists_iter())        \n",
        "\n",
        "    def calc_idf(self,list_of_tokens):\n",
        "        \"\"\"\n",
        "        This function calculate the idf values according to the BM25 idf formula for each term in the query.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        query: list of token representing the query. For example: ['look', 'blue', 'sky']\n",
        "        \n",
        "        Returns:\n",
        "        -----------\n",
        "        idf: dictionary of idf scores. As follows: \n",
        "                                                    key: term\n",
        "                                                    value: bm25 idf score\n",
        "        \"\"\"        \n",
        "        idf = {}        \n",
        "        for term in list_of_tokens:            \n",
        "            if term in self.index.df.keys():\n",
        "                n_ti = self.index.df[term]\n",
        "                idf[term] = math.log(1 + (self.N - n_ti + 0.5) / (n_ti + 0.5))\n",
        "            else:\n",
        "                pass                             \n",
        "        return idf\n",
        "        \n",
        "\n",
        "    def search(self, queries,N=3):\n",
        "        \"\"\"\n",
        "        This function calculate the bm25 score for given query and document.\n",
        "        We need to check only documents which are 'candidates' for a given query. \n",
        "        This function return a dictionary of scores as the following:\n",
        "                                                                    key: query_id\n",
        "                                                                    value: a ranked list of pairs (doc_id, score) in the length of N.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        query: list of token representing the query. For example: ['look', 'blue', 'sky']\n",
        "        doc_id: integer, document id.\n",
        "        \n",
        "        Returns:\n",
        "        -----------\n",
        "        score: float, bm25 score.\n",
        "        \"\"\"\n",
        "        # YOUR CODE HERE\n",
        "        dictionary = {}\n",
        "        for q in queries.keys():\n",
        "          self.idf = self.calc_idf(queries[q])\n",
        "          documents = get_candidate_documents(queries[q],self.index,self.words,self.pls)\n",
        "          lst = []\n",
        "          for doc_id in documents:\n",
        "            score = self._score(queries[q], doc_id)\n",
        "            tuple1 = (doc_id,score)\n",
        "            lst.append(tuple1)\n",
        "          lst.sort(key=lambda tup: tup[1], reverse=True) \n",
        "          dictionary[q]=lst[:N]\n",
        "        \n",
        "        return dictionary\n",
        "\n",
        "      \n",
        "       \n",
        "\n",
        "\n",
        "    def _score(self, query, doc_id):\n",
        "        \"\"\"\n",
        "        This function calculate the bm25 score for given query and document.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        query: list of token representing the query. For example: ['look', 'blue', 'sky']\n",
        "        doc_id: integer, document id.\n",
        "        \n",
        "        Returns:\n",
        "        -----------\n",
        "        score: float, bm25 score.\n",
        "        \"\"\"        \n",
        "        score = 0.0        \n",
        "        doc_len = DL[str(doc_id)]        \n",
        "             \n",
        "        for term in query:\n",
        "            if term in self.index.term_total.keys():                \n",
        "                term_frequencies = dict(self.pls[self.words.index(term)])                \n",
        "                if doc_id in term_frequencies.keys():            \n",
        "                    freq = term_frequencies[doc_id]\n",
        "                    numerator = self.idf[term] * freq * (self.k1 + 1)\n",
        "                    denominator = freq + self.k1 * (1 - self.b + self.b * doc_len / self.AVGDL)\n",
        "                    score += (numerator / denominator)\n",
        "        return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 370,
      "metadata": {
        "id": "4tQ7pM7oGyAe"
      },
      "outputs": [],
      "source": [
        "bm25_title = BM25_from_index(idx_title)\n",
        "bm25_queries_score_train = bm25_title.search(cran_txt_query_text_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 371,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "zQXVP_4MUeuN",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "89aa73dbfebedd3398857b7d593dc502",
          "grade": true,
          "grade_id": "cell-5e559c1bd3d86a60",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "#tests\n",
        "assert len(bm25_queries_score_train)==180\n",
        "assert 0 not in bm25_queries_score_train.keys()\n",
        "assert len(bm25_queries_score_train[5])==3\n",
        "assert bm25_queries_score_train[172][0][1] != bm25_queries_score_train[172][1][1]\n",
        "assert bm25_queries_score_train[14][0][1] != bm25_queries_score_train[172][1][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF6VVM9KHC1N"
      },
      "source": [
        "## Task 3: Using weights of title and body scores (15 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvJcS4AfHkT5"
      },
      "source": [
        "Reminder: we are building on the training set.\n",
        "Later you need to optimize the parameters and run the best parameters on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ek3G_fS61s8w"
      },
      "source": [
        "Now we will experiment with two sets of results. \n",
        "The first corresponds to results from the title index. \n",
        "The second corresponds to the results from the body index.\n",
        "\n",
        "We need to merge them into a single result set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY9tftQ9WNn0"
      },
      "source": [
        "**YOUR TASK (10 POINTS):** Complete the implementation of `merge_results`.\n",
        "This function merge and sort documents retrieved by its weighte score (e.g., title and body).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 372,
      "metadata": {
        "deletable": false,
        "id": "Oz1yjX5z2MmJ",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "293aa48e12f1e49163cf2a4f5cde5ca9",
          "grade": false,
          "grade_id": "cell-98093096905e84d7",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "from functools import reduce\n",
        "\n",
        "def merge_results(title_scores,body_scores,title_weight=0.5,text_weight=0.5,N = 3):    \n",
        "    \"\"\"\n",
        "    This function merge and sort documents retrieved by its weighte score (e.g., title and body). \n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    title_scores: a dictionary build upon the title index of queries and tuples representing scores as follows: \n",
        "                                                                            key: query_id\n",
        "                                                                            value: list of pairs in the following format:(doc_id,score)\n",
        "                \n",
        "    body_scores: a dictionary build upon the body/text index of queries and tuples representing scores as follows: \n",
        "                                                                            key: query_id\n",
        "                                                                            value: list of pairs in the following format:(doc_id,score)\n",
        "    title_weight: float, for weigted average utilizing title and body scores\n",
        "    text_weight: float, for weigted average utilizing title and body scores\n",
        "    N: Integer. How many document to retrieve. This argument is passed to topN function. By default N = 3, for the topN function. \n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    dictionary of querires and topN pairs as follows:\n",
        "                                                        key: query_id\n",
        "                                                        value: list of pairs in the following format:(doc_id,score). \n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    dictionary = {}\n",
        "    title_keys = title_scores.keys()\n",
        "    body_keys = body_scores.keys()\n",
        "    queries_ids = set (list(title_keys) + list(body_keys))\n",
        "    \n",
        "    for query_id in queries_ids :\n",
        "        lst_title_value = title_scores[query_id]\n",
        "        lst_body_value = body_scores[query_id]\n",
        "        lst = []\n",
        "        for tup in lst_title_value : \n",
        "          score = 0\n",
        "          found = False\n",
        "          for t in lst_body_value : # intersection\n",
        "            if t[0] == tup[0]:\n",
        "              score = title_weight*tup[1] + text_weight*t[1]\n",
        "              lst.append((tup[0] ,score))\n",
        "              found = True\n",
        "              break\n",
        "          if not found : # if just in title\n",
        "            lst.append((tup[0],title_weight*tup[1]))\n",
        "        for tup2 in lst_body_value :\n",
        "          found = False\n",
        "          for t in lst_title_value :\n",
        "            if t[0] == tup2[0]:\n",
        "              found = True\n",
        "              break\n",
        "          if not found : #if just in body\n",
        "            lst.append((tup2[0],text_weight*tup2[1]))\n",
        "        lst.sort(key=lambda tup: tup[1], reverse=True)\n",
        "        lst = lst[:N]\n",
        "        dictionary[query_id] = lst\n",
        "    return dictionary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 373,
      "metadata": {
        "id": "BPrpxaW1WtPF"
      },
      "outputs": [],
      "source": [
        "bm25_title = BM25_from_index(idx_title)\n",
        "bm25_body = BM25_from_index(idx_body)\n",
        "\n",
        "bm25_queries_score_train_title = bm25_title.search(cran_txt_query_text_train)\n",
        "bm25_queries_score_train_body = bm25_body.search(cran_txt_query_text_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 374,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "xHD8GBBZX_RL",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a093ed16a1bafce919e8124afb16f058",
          "grade": true,
          "grade_id": "cell-b79a606cd53bb733",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "#tests\n",
        "w1,w2 = 0.5, 0.5\n",
        "w3,w4 = 0.25,0.75\n",
        "half_and_half = merge_results(bm25_queries_score_train_title,bm25_queries_score_train_body,w1,w2)   \n",
        "assert len(half_and_half[2]) == 3\n",
        "assert type(half_and_half) == dict\n",
        "assert type(half_and_half[2]) == list\n",
        "assert len(half_and_half) == 180\n",
        "assert half_and_half[2][0][1] == 0.5 * (bm25_queries_score_train_title[2][-1][1]+ bm25_queries_score_train_body[2][0][1])       \n",
        "quarter_and_three_quarters = merge_results(bm25_queries_score_train_title,bm25_queries_score_train_body,0.25,0.75) \n",
        "assert quarter_and_three_quarters[2][0][1] == (w3 * bm25_queries_score_train_title[2][-1][1] + w4 * bm25_queries_score_train_body[2][0][1])\n",
        "assert {k for k,v in half_and_half[16]} != {k for k,v in quarter_and_three_quarters[16]}\n",
        "assert len({k for k,v in half_and_half[16]}.union({k for k,v in quarter_and_three_quarters[16]})) < (len({k for k,v in half_and_half[16]}) + len({k for k,v in quarter_and_three_quarters[16]}))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "finBStQSEfQO"
      },
      "source": [
        "**YOUR TASK (5 POINTS):** provide three examples of mistakes that the model is making and explanations for why, and describe how you will change the model based on these observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 375,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "AyUzoE--JLjq",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b0c0e38ce983a84e284885057b0ff8de",
          "grade": true,
          "grade_id": "cell-5a10abd02a1533e8",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# YOUR EXAMPLES HERE #\n",
        "# there is a problem if i try to merge list if doc_id in list of titles but not in list of body or contrary / reverse\n",
        "# there is a problem that i forget to sort the list of query_id's value by score\n",
        "assert half_and_half[3][1][1] == 0.5 * (bm25_queries_score_train_title[3][0][1])\n",
        "assert half_and_half[1][-1][1] == 0.5 * (bm25_queries_score_train_body[1][1][1])\n",
        "assert quarter_and_three_quarters[10][0][1] == 0.75 * (bm25_queries_score_train_body[10][0][1])\n",
        "assert True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6pw-n8hDzZy"
      },
      "source": [
        "## Task 4: Implement several evaluation metrics (25 points)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kDVohylRRYx"
      },
      "source": [
        "At this task, you will need to write the multiple metrics **(without using pre-defined packages)** and evaluate the results on the test set.\n",
        "\n",
        "\n",
        "Reminder: \n",
        "* `carn_queries_train` holds the queries for the training set\n",
        "* `cran_qry_rel_data_train` holds the relevant data for each query in the training set.\n",
        "* `carn_queries_test` holds the queries for the test set\n",
        "* `cran_qry_rel_data_test` holds the relevant data for each query in the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 376,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "PD0bhfyZBbHI",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3a7d7a234651a479f0c2433c8f50fdbb",
          "grade": false,
          "grade_id": "cell-869dad25b4aa548e",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def intersection(l1,l2):      \n",
        "    \"\"\"\n",
        "    This function perform an intersection between two lists.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    l1: list of documents. Each element is a doc_id.\n",
        "    l2: list of documents. Each element is a doc_id.\n",
        "\n",
        "    Returns:\n",
        "    ----------\n",
        "    list with the intersection (without duplicates) of l1 and l2\n",
        "    \"\"\"\n",
        "    return list(set(l1)&set(l2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZRU9_99cyNG"
      },
      "source": [
        "**YOUR TASK (25 POINTS):** Complete the implementation of the below functions:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 377,
      "metadata": {
        "deletable": false,
        "id": "sp5QUYW2uVK1",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "650c73e94a796808f212d8cd38253476",
          "grade": false,
          "grade_id": "cell-adeeada566742c26",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def recall_at_k(true_list,predicted_list,k=40):\n",
        "    \"\"\"\n",
        "    This function calculate the recall@k metric.\n",
        "\n",
        "    Parameters\n",
        "    -----------\n",
        "    true_list: list of relevant documents. Each element is a doc_id.\n",
        "    predicted_list: sorted list of documents predicted as relevant. Each element is a doc_id. Sorted is performed by relevance score\n",
        "    k: integer, a number to slice the length of the predicted_list\n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    float, recall@k with 3 digits after the decimal point.\n",
        "    \"\"\" \n",
        "     # YOUR CODE HERE\n",
        "    predicted_list = predicted_list[:k]\n",
        "    tp = 0\n",
        "    for i in true_list :\n",
        "      if i in predicted_list :\n",
        "        tp+=1\n",
        "    return float(\"{:.3f}\".format(tp / len(true_list)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 378,
      "metadata": {
        "deletable": false,
        "id": "gvq9EOMNuXJ1",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f5e3de835e8a832f6eb0b62aa832badd",
          "grade": false,
          "grade_id": "cell-d42c244b918cd9be",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def precision_at_k(true_list,predicted_list,k=40):    \n",
        "    \"\"\"\n",
        "    This function calculate the precision@k metric.\n",
        "\n",
        "    Parameters\n",
        "    -----------\n",
        "    true_list: list of relevant documents. Each element is a doc_id.\n",
        "    predicted_list: sorted list of documents predicted as relevant. Each element is a doc_id. Sorted is performed by relevance score\n",
        "    k: integer, a number to slice the length of the predicted_list\n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    float, precision@k with 3 digits after the decimal point.\n",
        "    \"\"\"      \n",
        "    # YOUR CODE HERE\n",
        "    predicted_list = predicted_list[:k]\n",
        "    tp = 0\n",
        "    for i in true_list :\n",
        "      if i in predicted_list :\n",
        "        tp+=1\n",
        "    return float(\"{:.3f}\".format(tp / len(predicted_list)))\n",
        "    \n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 379,
      "metadata": {
        "deletable": false,
        "id": "BCC78MnBuidc",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e58c153c887ad8c3b4e24b6ef22a5d66",
          "grade": false,
          "grade_id": "cell-3707bcda91156fc6",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def r_precision(true_list,predicted_list):\n",
        "    \"\"\"\n",
        "    This function calculate the r-precision metric. No `k` parameter is used.\n",
        "\n",
        "    Parameters\n",
        "    -----------\n",
        "    true_list: list of relevant documents. Each element is a doc_id.\n",
        "    predicted_list: sorted list of documents predicted as relevant. Each element is a doc_id. Sorted is performed by relevance score    \n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    float, r-precision with 3 digits after the decimal point.\n",
        "    \"\"\"    \n",
        "    # YOUR CODE HERE\n",
        "    counter = 0\n",
        "    tp = 0\n",
        "    for i in predicted_list :\n",
        "      if i in true_list :\n",
        "        tp+=1\n",
        "    return float(\"{:.3f}\".format(recall_at_k(true_list,predicted_list,k=len(true_list))) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 380,
      "metadata": {
        "deletable": false,
        "id": "yDDYMdP0ujej",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "39340b25127170e8c154e1ce11dd888e",
          "grade": false,
          "grade_id": "cell-5812899240394e72",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def reciprocal_rank_at_k(true_list,predicted_list,k=40):\n",
        "    \"\"\"\n",
        "    This function calculate the reciprocal_rank@k metric.        \n",
        "    Parameters\n",
        "    -----------\n",
        "    true_list: list of relevant documents. Each element is a doc_id.\n",
        "    predicted_list: sorted list of documents predicted as relevant. Each element is a doc_id. Sorted is performed by relevance score\n",
        "    k: integer, a number to slice the length of the predicted_list\n",
        "    \n",
        "    Returns:\n",
        "    -----------    \n",
        "    float, reciprocal rank@k with 3 digits after the decimal point.\n",
        "    \"\"\"   \n",
        "     # YOUR CODE HERE\n",
        "    predicted_list = predicted_list[:k]\n",
        "    mean = 0\n",
        "    for i in predicted_list :\n",
        "      if i in true_list:\n",
        "        rank = 1 + predicted_list.index(i)\n",
        "        return 1/rank\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 381,
      "metadata": {
        "deletable": false,
        "id": "6wF5SRDCuja0",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "dcdf961bb1e282264951a9ad71ed70c0",
          "grade": false,
          "grade_id": "cell-ffff41015cc64657",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def fallout_rate(true_list,predicted_list,k=40):    \n",
        "    \"\"\"\n",
        "    This function calculate the fallout_rate@k metric.\n",
        "\n",
        "    Parameters\n",
        "    -----------\n",
        "    true_list: list of relevant documents. Each element is a doc_id.\n",
        "    predicted_list: sorted list of documents predicted as relevant. Each element is a doc_id. Sorted is performed by relevance score\n",
        "    k: integer, a number to slice the length of the predicted_list\n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    float, fallout_rate@k with 3 digits after the decimal point.\n",
        "    \"\"\" \n",
        "    # YOUR CODE HERE\n",
        "    predicted_list = predicted_list[:k]\n",
        "    tn = 0\n",
        "    fp = 0\n",
        "    \n",
        "    for i in predicted_list:\n",
        "      if i not in true_list:\n",
        "        fp+=1\n",
        "    return float(\"{:.3f}\".format(fp / (1000 - len(true_list))))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 382,
      "metadata": {
        "deletable": false,
        "id": "VvxeVXaUu4VU",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "979052b4025f51df641b08d461cdb872",
          "grade": false,
          "grade_id": "cell-b590eb282d0bb526",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def f_score(true_list,predicted_list,k=40):\n",
        "    \"\"\"\n",
        "    This function calculate the f_score@k metric.\n",
        "\n",
        "    Parameters\n",
        "    -----------\n",
        "    true_list: list of relevant documents. Each element is a doc_id.\n",
        "    predicted_list: sorted list of documents predicted as relevant. Each element is a doc_id. Sorted is performed by relevance score\n",
        "    k: integer, a number to slice the length of the predicted_list\n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    float, f-score@k with 3 digits after the decimal point.\n",
        "    \"\"\"   \n",
        "    # YOUR CODE HERE\n",
        "    precision = precision_at_k(true_list,predicted_list,k)\n",
        "    recall = recall_at_k(true_list,predicted_list,k)\n",
        "    if precision + recall == 0 :\n",
        "      return 0\n",
        "    return 2 * (precision * recall) / (precision + recall)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 383,
      "metadata": {
        "deletable": false,
        "id": "vxZI0rSvu4PM",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c135b52c829940d52cd0037e9a50b342",
          "grade": false,
          "grade_id": "cell-60fcbcd448af3f0d",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def average_precision(true_list,predicted_list,k=40):\n",
        "    \"\"\"\n",
        "    This function calculate the average_precision@k metric.(i.e., precision in every recall point).     \n",
        "\n",
        "    Parameters\n",
        "    -----------\n",
        "    true_list: list of relevant documents. Each element is a doc_id.\n",
        "    predicted_list: sorted list of documents predicted as relevant. Each element is a doc_id. Sorted is performed by relevance score\n",
        "    k: integer, a number to slice the length of the predicted_list\n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    float, average precision@k with 3 digits after the decimal point.\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    result = 0\n",
        "    predicted_list = predicted_list[:k]\n",
        "    r = 0\n",
        "    for index in range(0,k):\n",
        "      if predicted_list[index] in true_list :\n",
        "        result+= precision_at_k(true_list,predicted_list,k=index+1)\n",
        "        r+=1\n",
        "    if r==0:\n",
        "      return 0\n",
        "    return round(result/r , 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 384,
      "metadata": {
        "deletable": false,
        "id": "vgZ0rJcA-i8d",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f209ce095e0bf30b016282c78e0c064c",
          "grade": false,
          "grade_id": "cell-3abfa5e74074bc52",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        " def ndcg_at_k(true_tuple_list,predicted_list,k=40):\n",
        "    \"\"\"\n",
        "    This function calculate the ndcg@k metric.\n",
        "\n",
        "    Parameters\n",
        "    -----------\n",
        "    true_list: list of relevant documents. Each element is a doc_id.\n",
        "    predicted_list: sorted list of documents predicted as relevant. Each element is a doc_id. Sorted is performed by relevance score\n",
        "    k: integer, a number to slice the length of the predicted_list\n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    float, ndcg@k with 3 digits after the decimal point.\n",
        "    \"\"\" \n",
        "    # YOUR CODE HERE\n",
        "    predicted_list = predicted_list[:k]\n",
        "    rel={}\n",
        "    for tup in true_tuple_list:\n",
        "      rel[tup[0]]=tup[1]\n",
        "    keys = rel.keys()\n",
        "    result1 = 0\n",
        "    lst = []\n",
        "    \n",
        "    for i in range(len(predicted_list)) :\n",
        "      doc_id = predicted_list[i]\n",
        "      if doc_id in keys :\n",
        "        if i==0:\n",
        "          result1+=rel[doc_id]\n",
        "        else:\n",
        "          result1 += rel[doc_id] / math.log(i+1)\n",
        "        \n",
        "        tup = (doc_id,rel[doc_id])\n",
        "        lst.append(tup)\n",
        "      else:\n",
        "        tup = (doc_id,0)\n",
        "        lst.append(tup)\n",
        "    result2 = 0\n",
        "    lst.sort(key=lambda tup: tup[1], reverse=True) \n",
        "    for i in range(len(lst)):\n",
        "      doc_id = lst[i][0]\n",
        "      if doc_id in keys :\n",
        "        if i==0:\n",
        "            result2 += rel[doc_id]\n",
        "        else:\n",
        "            result2 += rel[doc_id] / math.log(i+1)\n",
        "\n",
        "    if result1 == 0 or result2==0:\n",
        "      return 0\n",
        "    return result1 / result2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWv80dPZdQMM"
      },
      "source": [
        "Next, we define `evaluate`. A function that calculates multiple metrics and returns a dictionary with metrics scores across different queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 385,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "RIzXGlg2-5lu",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "278620aceda470a22294f0bcee1cef51",
          "grade": false,
          "grade_id": "cell-b7da3d892da52b5d",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def evaluate(true_relevancy,predicted_relevancy,k,print_scores=True):\n",
        "    \"\"\"\n",
        "    This function calculates multiple metrics and returns a dictionary with metrics scores across different queries.\n",
        "    Parameters\n",
        "    -----------\n",
        "    true_relevancy: list of tuples indicating the relevancy score for a query. Each element corresponds to a query.\n",
        "    Example of a single element in the list: \n",
        "                                            (3, {'question': ' what problems of heat conduction in composite slabs have been solved so far . ',\n",
        "                                            'relevance_assessments': [(5, 3), (6, 3), (90, 3), (91, 3), (119, 3), (144, 3), (181, 3), (399, 3), (485, 1)]})\n",
        "     \n",
        "    predicted_relevancy: a dictionary of the list. Each key represents the query_id. The value of the dictionary is a sorted list of relevant documents and their scores.\n",
        "                         The list is sorted by the score.  \n",
        "    Example:\n",
        "            key: 1\n",
        "            value: [(13, 17.256625), (486, 13.539465), (12, 9.957595), (746, 9.599499999999999), (51, 9.171265), .....]            \n",
        "            \n",
        "    k: integer, a number to slice the length of the predicted_list\n",
        "    \n",
        "    print_scores: boolean, enable/disable a print of the mean value of each metric.\n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    a dictionary of metrics scores as follows: \n",
        "                                                        key: metric name\n",
        "                                                        value: list of metric scores. Each element corresponds to a given query.\n",
        "    \"\"\"    \n",
        "\n",
        "    recall_lst = []\n",
        "    precision_lst = []\n",
        "    f_score_lst = []\n",
        "    r_precision_lst = []\n",
        "    reciprocal_rank_lst = []\n",
        "    avg_precision_lst = []\n",
        "    fallout_rate_lst = []\n",
        "    ndcg_lst = []\n",
        "    metrices = {'recall@k':recall_lst,\n",
        "                'precision@k':precision_lst,\n",
        "                'f_score@k': f_score_lst,\n",
        "                'r-precision': r_precision_lst,\n",
        "                'MRR@k':reciprocal_rank_lst,\n",
        "                'MAP@k':avg_precision_lst,\n",
        "                'fallout@k':fallout_rate_lst,\n",
        "                'ndcg@k':ndcg_lst}\n",
        "    \n",
        "    for query_id, query_info in tqdm(true_relevancy):\n",
        "        predicted = [doc_id for doc_id, score in predicted_relevancy[query_id]]    \n",
        "        ground_true = [int(doc_id) for doc_id, score in query_info['relevance_assessments']]\n",
        "    \n",
        "        recall_lst.append(recall_at_k(ground_true,predicted,k=k))\n",
        "        precision_lst.append(precision_at_k(ground_true,predicted,k=k))\n",
        "        f_score_lst.append(f_score(ground_true,predicted,k=k))\n",
        "        r_precision_lst.append(r_precision(ground_true,predicted))\n",
        "        reciprocal_rank_lst.append(reciprocal_rank_at_k(ground_true,predicted,k=k))\n",
        "        avg_precision_lst.append(average_precision(ground_true,predicted,k=k))\n",
        "        fallout_rate_lst.append(fallout_rate(ground_true,predicted,k=k))\n",
        "        ndcg_lst.append(ndcg_at_k(query_info['relevance_assessments'],predicted,k=k))\n",
        "\n",
        "    if print_scores:\n",
        "        for name,values in metrices.items():\n",
        "                print(name,sum(values)/len(values))\n",
        "\n",
        "    return metrices    \n",
        "    \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0PjSWWmHlSq"
      },
      "source": [
        "Let's examine the metrices scores according to the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 386,
      "metadata": {
        "id": "nHf9_Peovk11"
      },
      "outputs": [],
      "source": [
        "# For demonstration we will use N=100. \n",
        "\n",
        "N = 100\n",
        "bm25_queries_score_test_title = bm25_title.search(cran_txt_query_text_test,N=N)\n",
        "bm25_queries_score_test_body = bm25_body.search(cran_txt_query_text_test,N=N)\n",
        "half_and_half_test = merge_results(bm25_queries_score_test_title,bm25_queries_score_test_body,N=N)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 387,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "_2SD2TxBxJXQ",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "14f00bb5f8d9f5501d062b89861d31d3",
          "grade": true,
          "grade_id": "cell-536d1d9424c720ae",
          "locked": true,
          "points": 25,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "#tests\n",
        "ground_true = [int(doc_id) for doc_id, score in cran_qry_rel_data_test[0][1]['relevance_assessments']]\n",
        "predicted = [doc_id for doc_id, score in half_and_half_test[181]] \n",
        "assert precision_at_k(ground_true,predicted,k=20) == 0.15\n",
        "assert recall_at_k(ground_true,predicted,k=20) == 0.5\n",
        "assert reciprocal_rank_at_k(ground_true,predicted,k=4) == 0.0\n",
        "assert reciprocal_rank_at_k(ground_true,predicted,k=5) == 0.2\n",
        "assert r_precision(ground_true,predicted) == 0.333\n",
        "assert f_score(ground_true,predicted,3) == f_score(ground_true,predicted,4)\n",
        "assert f_score(ground_true,predicted,4) < f_score(ground_true,predicted,5)\n",
        "assert ndcg_at_k(cran_qry_rel_data_test[0][1]['relevance_assessments'],predicted,4) == reciprocal_rank_at_k(ground_true,predicted,k=4)\n",
        "assert ndcg_at_k(cran_qry_rel_data_test[0][1]['relevance_assessments'],predicted,5) > 0\n",
        "assert fallout_rate(ground_true,predicted,4000) == fallout_rate(ground_true,predicted,40000)\n",
        "assert fallout_rate(ground_true,predicted,1) == 0.001\n",
        "assert average_precision(ground_true,predicted,5) == 0.2\n",
        "ground_true = [int(doc_id) for doc_id, score in cran_qry_rel_data_test[1][1]['relevance_assessments']]\n",
        "predicted = [doc_id for doc_id, score in half_and_half_test[182]] \n",
        "assert precision_at_k(ground_true,predicted,k=4) == 0.5\n",
        "assert recall_at_k(ground_true,predicted,k=4) == 0.667\n",
        "assert reciprocal_rank_at_k(ground_true,predicted,k=4) == 1\n",
        "assert r_precision(ground_true,predicted) == 0.667\n",
        "assert ndcg_at_k(cran_qry_rel_data_test[1][1]['relevance_assessments'],predicted,16) == 1\n",
        "assert average_precision(ground_true,predicted,17) == round((precision_at_k(ground_true,predicted,k=1)+precision_at_k(ground_true,predicted,k=2)+precision_at_k(ground_true,predicted,k=17))/3,3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "di27wD6RPw2z"
      },
      "source": [
        "## Task 5: Optimization (5 points)\n",
        "Optimize your ranking function for MAP@N using the training set.\n",
        "Try 2 different (not default) combinations of the free parameters and try them with 2 different weight (not included equal weights) on the merge of title and body indecis. \n",
        "\n",
        "**Using the training set only.**\n",
        "\n",
        "Than, choose you final model and evalute it using MAP@30 on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYSaFQydI7Jy"
      },
      "source": [
        "**YOUR TASK (5 POINTS):** Complete the implementation of the `grid_search_models`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 388,
      "metadata": {
        "deletable": false,
        "id": "o93lo5aEPxsS",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d9064a11882b04a1fadb641582a93c44",
          "grade": false,
          "grade_id": "cell-35bb2b37793c0475",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def grid_search_models(data,true_relevancy,bm25_param_list,w_list,N,idx_title,idx_body):\n",
        "\n",
        "    \"\"\"\n",
        "    This function is performing a grid search upon different combination of parameters.\n",
        "    The parameters can be BM25 parameters (i.e., bm25_param_list) or different weights (i.e., w_list).\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    data: dictionary as follows: \n",
        "                            key: query_id\n",
        "                            value: list of tokens\n",
        "\n",
        "    true_relevancy: list of tuples indicating the relevancy score for a query. Each element corresponds to a query.\n",
        "    Example of a single element in the list: \n",
        "                                            (3, {'question': ' what problems of heat conduction in composite slabs have been solved so far . ',\n",
        "                                            'relevance_assessments': [(5, 3), (6, 3), (90, 3), (91, 3), (119, 3), (144, 3), (181, 3), (399, 3), (485, 1)]})     \n",
        "        \n",
        "    bm25_param_list: list of tuples. Each tuple represent (k,b1) values.\n",
        "\n",
        "    w_list: list of tuples. Each tuple represent (title_weight,body_weight).    \n",
        "    N: Integer. How many document to retrieve. \n",
        "    \n",
        "    idx_title: index build upon titles\n",
        "    idx_body:  index build upon bodies\n",
        "    ----------\n",
        "    return: dictionary as follows:\n",
        "                            key: tuple indiciating the parameters examined in the model (k1,b,title_weight,body_weight)\n",
        "                            value: MAP@N score\n",
        "    \"\"\"\n",
        "    models = {}\n",
        "    # YOUR CODE HERE\n",
        "    for i in bm25_param_list :\n",
        "      for j in w_list :\n",
        "          bm25_title = BM25_from_index(idx_title,i[0],i[1])\n",
        "          bm25_body = BM25_from_index(idx_body,i[0],i[1])\n",
        "          bm25_queries_score_train_title = bm25_title.search(cran_txt_query_text_train)\n",
        "          bm25_queries_score_train_body = bm25_body.search(cran_txt_query_text_train)\n",
        "          merged = merge_results(bm25_queries_score_train_title , bm25_queries_score_train_body ,j[0] , j[1] ) \n",
        "          key = (i[0],i[1],j[0],j[1])\n",
        "          true_list = [int(doc_id) for doc_id, score in true_relevancy[0][1]['relevance_assessments']]\n",
        "          value = average_precision(true_list , list(merged.keys()) , N)\n",
        "          models[key]=value\n",
        "    return models\n",
        "\n",
        "\n",
        "\n",
        "parameters_list = [(1.4,0.8),(1.6,0.7)]\n",
        "weights_list = [(0.35,0.65),(0.2,0.8)]\n",
        "N = 30\n",
        "grid_models = grid_search_models(cran_txt_query_text_train,cran_qry_rel_data_train,parameters_list,weights_list,N,idx_title,idx_body)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 389,
      "metadata": {
        "id": "lv0BD_x6HEC5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb0b7390-7af1-4f28-d91b-55d7609f2cda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [00:00<00:00, 1819.76it/s]\n"
          ]
        }
      ],
      "source": [
        "#tests\n",
        "# get key for max value in dictioary\n",
        "k1,b,title_w,body_w = max(grid_models, key=grid_models.get)\n",
        "\n",
        "bm25_title = BM25_from_index(idx_title,k1=k1,b=b)        \n",
        "bm25_body = BM25_from_index(idx_body,k1=k1,b=b)\n",
        "bm25_queries_score_test_title = bm25_title.search(cran_txt_query_text_test,N=N)\n",
        "bm25_queries_score_test_body = bm25_body.search(cran_txt_query_text_test,N=N)\n",
        "merge_res = merge_results(bm25_queries_score_test_title,bm25_queries_score_test_body,title_weight=title_w,text_weight=body_w,N=N)\n",
        "test_metrices = evaluate(cran_qry_rel_data_test,merge_res,k=N,print_scores=False) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 390,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "En3cx-mOsyaU",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "9def801e372ff409f40373ce684198a1",
          "grade": true,
          "grade_id": "cell-286bed7f8b35d705",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "       \n",
        "assert round(sum(test_metrices['MAP@k'])/len(test_metrices['MAP@k']),3) > 0.52\n",
        "assert len(test_metrices['MAP@k']) == 45\n",
        "assert max(test_metrices['MAP@k']) == 1\n",
        "assert min(test_metrices['MAP@k']) == 0\n",
        "assert len(grid_models) >= 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7KfBOc5c2h2"
      },
      "source": [
        "## Task 6: Plots (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOjg0io8KJAr"
      },
      "source": [
        "**YOUR TASK (5 POINTS):** Complete the implementation of `plot_metric_with_differnt_k_values`. \n",
        "Plot different values of precision and recall (utilizing different k values) for a given query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 391,
      "metadata": {
        "deletable": false,
        "id": "F32lWS1-C7QC",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ca88ea921a7bd91e5b94013eee4d4756",
          "grade": false,
          "grade_id": "cell-457393c4a35595f6",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def plot_metric_with_differnt_k_values(true_relevancy,predicted_relevancy,metrices_names,k_values):    \n",
        "    \"\"\"\n",
        "    This function plot a for each given metric its value depands on k_values as line chart.\n",
        "    This function does not return any value.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    true_relevancy: list of tuples indicating the relevancy score for a query. Each element corresponds to a query.\n",
        "    Example of a single element in the list: \n",
        "                                            (3, {'question': ' what problems of heat conduction in composite slabs have been solved so far . ',\n",
        "                                            'relevance_assessments': [(5, 3), (6, 3), (90, 3), (91, 3), (119, 3), (144, 3), (181, 3), (399, 3), (485, 1)]})\n",
        "\n",
        "    predicted_relevancy: a dictionary of the list. Each key represents the query_id. The value of the dictionary is a sorted list of relevant documents and their scores.\n",
        "                         The list is sorted by the score.  \n",
        "    Example:\n",
        "            key: 1\n",
        "            value: [(13, 17.256625), (486, 13.539465), (12, 9.957595), (746, 9.599499999999999), (51, 9.171265), .....]\n",
        "\n",
        "    metrices_names: list of string representing the metrices to plot. For example: ['precision@k','recall@k','f_score@k']\n",
        "\n",
        "    k_values: list of integer of different k values. For example [1,3,5]\n",
        "\n",
        "    returns: \n",
        "    plot values in format : \n",
        "    \n",
        "    statistics[metric_name] = [values , k_values]\n",
        "\n",
        "    \"\"\"\n",
        "    statistics = {}\n",
        "    for metric_name in metrices_names:\n",
        "    # YOUR CODE HERE\n",
        "      values = []\n",
        "      for k in k_values:\n",
        "        counter = 0\n",
        "        for key in predicted_relevancy.keys():\n",
        "          pr = [ x[0] for x in predicted_relevancy[key] ]\n",
        "          tr_key = [i for i in true_relevancy if i[0]==key]\n",
        "          tr = [ y[0] for y in tr_key[0][1]['relevance_assessments'] ]\n",
        "          if metric_name == 'precision@k':\n",
        "            counter+= precision_at_k(tr,pr,k)\n",
        "          if metric_name == 'recall@k':\n",
        "            counter+= recall_at_k(tr,pr,k)\n",
        "          if metric_name == 'f_score@k':\n",
        "            counter+= f_score(tr,pr,k)\n",
        "        values.append( counter )\n",
        "      statistics[metric_name] = [values,k_values]\n",
        "    return statistics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjMdBJuOKWZ7"
      },
      "source": [
        "**YOUR TASK (5 POINTS):** Complete the implementation of `plot_metric_different_quieries`. \n",
        "Plot different values of metrices (utilizing same k values) for different queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 392,
      "metadata": {
        "deletable": false,
        "id": "kaSZKHNyUQAT",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4648afcfe09bd7ac49eab5c64380035d",
          "grade": false,
          "grade_id": "cell-c7df48f68a875a07",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def plot_metric_different_quieries(true_relevancy,predicted_relevancy,metrices_names,k):\n",
        "    \"\"\"\n",
        "    This function plot for each given metric its value across all queries.\n",
        "    This function does not return any value.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    true_relevancy: list of tuples indicating the relevancy score for a query. Each element corresponds to a query.\n",
        "    Example of a single element in the list: \n",
        "                                            (3, {'question': ' what problems of heat conduction in composite slabs have been solved so far . ',\n",
        "                                            'relevance_assessments': [(5, 3), (6, 3), (90, 3), (91, 3), (119, 3), (144, 3), (181, 3), (399, 3), (485, 1)]})\n",
        "\n",
        "    predicted_relevancy: a dictionary of the list. Each key represents the query_id. The value of the dictionary is a sorted list of relevant documents and their scores.\n",
        "                         The list is sorted by the score.  \n",
        "    Example:\n",
        "            key: 1\n",
        "            value: [(13, 17.256625), (486, 13.539465), (12, 9.957595), (746, 9.599499999999999), (51, 9.171265), .....]\n",
        "\n",
        "    metrices_names: list of string representing the metrices to plot. For example: ['precision@k','recall@k','f_score@k']\n",
        "    k: integer, a number to slice the length of the predicted_list.\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    for metric_name in metrices_names:\n",
        "        # YOUR CODE HERE\n",
        "        lst = []\n",
        "        for key in predicted_relevancy.keys():\n",
        "          pr = [ x[0] for x in predicted_relevancy[key] ]\n",
        "          tr_key = [i for i in true_relevancy if i[0]==key]\n",
        "          tr = [ y[0] for y in tr_key[0][1]['relevance_assessments'] ]\n",
        "          if metric_name == 'precision@k':\n",
        "            lst.append( precision_at_k(tr,pr,k))\n",
        "          if metric_name == 'recall@k':\n",
        "            lst.append( recall_at_k(tr,pr,k))\n",
        "          if metric_name == 'f_score@k':\n",
        "            lst.append( f_score(tr,pr,k))\n",
        "          \n",
        "          \n",
        "        \n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 393,
      "metadata": {
        "id": "NwazcB5pHHG5"
      },
      "outputs": [],
      "source": [
        "k_values = [1,3,5,10,20,30,40]\n",
        "metrices_names = ['precision@k','recall@k','f_score@k','MRR@k','fallout@k']\n",
        "statistics_of_differnt_k = plot_metric_with_differnt_k_values(cran_qry_rel_data_test,merge_res,metrices_names,k_values=k_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 394,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "P63QRjsR4AJU",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a87f1de9dce9b5017cedc700754117e6",
          "grade": true,
          "grade_id": "cell-2d3970e3c54d1dcf",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "assert  all(statistics_of_differnt_k['precision@k'][0][i-1] > statistics_of_differnt_k['precision@k'][0][i] for i in range(1, len(statistics_of_differnt_k['precision@k'])))\n",
        "assert  all(statistics_of_differnt_k['recall@k'][0][i-1] < statistics_of_differnt_k['recall@k'][0][i] for i in range(1, len(statistics_of_differnt_k['recall@k'])))\n",
        "assert  all(statistics_of_differnt_k['MRR@k'][0][i-1] <= statistics_of_differnt_k['MRR@k'][0][i] for i in range(1, len(statistics_of_differnt_k['MRR@k'])))\n",
        "assert  all(statistics_of_differnt_k['fallout@k'][0][i-1] <= statistics_of_differnt_k['fallout@k'][0][i] for i in range(1, len(statistics_of_differnt_k['fallout@k'])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 395,
      "metadata": {
        "id": "J1ZkpIcnHKpW"
      },
      "outputs": [],
      "source": [
        "plot_metric_different_quieries(cran_qry_rel_data_test,merge_res,metrices_names,k=N)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 396,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "a58y0948xwwM",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f99eee2147e3fdb8d793e04e844a2377",
          "grade": true,
          "grade_id": "cell-36163022becd303e",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "assert True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "E4pIk7dG9YEu"
      },
      "execution_count": 396,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Qf028y32uxYx"
      ],
      "name": "assignment_4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": false,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}